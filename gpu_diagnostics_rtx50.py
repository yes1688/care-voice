#!/usr/bin/env python3
"""
RTX 50 ç³»åˆ—é€šç”¨ GPU è¨ºæ–·å·¥å…·
æ”¯æ´ RTX 50/40/30/20 ç³»åˆ—ï¼Œå‘ä¸‹å…¼å®¹åˆ° GTX 10 ç³»åˆ—
æª¢æ¸¬å’Œé©—è­‰ Ubuntu 24.04 + CUDA 12.8 + PyTorch nightly cu128 ç’°å¢ƒ
"""

import os
import sys
import json
import time
import traceback
import torch
import numpy as np

def print_header(title):
    """æ‰“å°æ¨™é¡Œ"""
    print("\n" + "=" * 70)
    print(f" {title}")
    print("=" * 70)

def print_section(title):
    """æ‰“å°ç« ç¯€æ¨™é¡Œ"""
    print(f"\nğŸ” {title}")
    print("-" * 50)

def check_basic_environment():
    """æª¢æŸ¥åŸºæœ¬ç’°å¢ƒ"""
    print_section("åŸºæœ¬ç’°å¢ƒæª¢æŸ¥")
    
    print(f"Python ç‰ˆæœ¬: {sys.version}")
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDA ç‰ˆæœ¬ (PyTorch): {torch.version.cuda}")
    print(f"cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
    
    # ç’°å¢ƒè®Šé‡
    print(f"CUDA_HOME: {os.environ.get('CUDA_HOME', 'Not set')}")
    print(f"CUDA_VISIBLE_DEVICES: {os.environ.get('CUDA_VISIBLE_DEVICES', 'Not set')}")
    print(f"TORCH_CUDA_ARCH_LIST: {os.environ.get('TORCH_CUDA_ARCH_LIST', 'Not set')}")

def check_cuda_availability():
    """æª¢æŸ¥ CUDA å¯ç”¨æ€§"""
    print_section("CUDA å¯ç”¨æ€§æª¢æŸ¥")
    
    try:
        cuda_available = torch.cuda.is_available()
        print(f"torch.cuda.is_available(): {cuda_available}")
        
        if cuda_available:
            device_count = torch.cuda.device_count()
            print(f"CUDA è¨­å‚™æ•¸é‡: {device_count}")
            
            current_device = torch.cuda.current_device()
            print(f"ç•¶å‰è¨­å‚™: {current_device}")
            
            return True
        else:
            print("âŒ CUDA ä¸å¯ç”¨")
            return False
            
    except Exception as e:
        print(f"âŒ CUDA æª¢æŸ¥å¤±æ•—: {e}")
        return False

def check_gpu_details():
    """æª¢æŸ¥ GPU è©³ç´°ä¿¡æ¯"""
    print_section("GPU è©³ç´°ä¿¡æ¯")
    
    if not torch.cuda.is_available():
        print("âŒ ç„¡ GPU å¯æª¢æŸ¥")
        return False
    
    rtx_series_found = False
    best_gpu_arch = 0
    
    for i in range(torch.cuda.device_count()):
        print(f"\nğŸ“± GPU {i}:")
        
        try:
            gpu_name = torch.cuda.get_device_name(i)
            props = torch.cuda.get_device_properties(i)
            
            print(f"   åç¨±: {gpu_name}")
            print(f"   è¨ˆç®—èƒ½åŠ›: {props.major}.{props.minor} (sm_{props.major}{props.minor})")
            print(f"   ç¸½è¨˜æ†¶é«”: {props.total_memory / 1024**3:.2f} GB")
            print(f"   å¤šè™•ç†å™¨æ•¸é‡: {props.multi_processor_count}")
            print(f"   æœ€å¤§åŸ·è¡Œç·’/å¡Š: {props.max_threads_per_block}")
            print(f"   æœ€å¤§å…±äº«è¨˜æ†¶é«”/å¡Š: {props.max_shared_memory_per_block / 1024:.1f} KB")
            print(f"   Warp å¤§å°: {props.warp_size}")
            
            # æª¢æŸ¥ RTX ç³»åˆ—æ”¯æ´ (å¤šæ¶æ§‹å…¼å®¹)
            is_supported = False
            gpu_series = "æœªçŸ¥"
            
            if props.major >= 12:  # RTX 50 ç³»åˆ— (sm_120+)
                is_supported = True
                gpu_series = "RTX 50 ç³»åˆ—"
                print(f"   âœ… {gpu_series} (sm_{props.major}{props.minor}) æª¢æ¸¬æˆåŠŸ! [æœ€æ–°æ¶æ§‹]")
            elif props.major == 8 and props.minor == 9:  # RTX 40 ç³»åˆ— (sm_89)
                is_supported = True
                gpu_series = "RTX 40 ç³»åˆ—"
                print(f"   âœ… {gpu_series} (sm_{props.major}{props.minor}) æª¢æ¸¬æˆåŠŸ!")
            elif props.major == 8 and props.minor == 6:  # RTX 30 ç³»åˆ— (sm_86)
                is_supported = True
                gpu_series = "RTX 30 ç³»åˆ—"
                print(f"   âœ… {gpu_series} (sm_{props.major}{props.minor}) æª¢æ¸¬æˆåŠŸ!")
            elif props.major == 7 and props.minor == 5:  # RTX 20 ç³»åˆ—/GTX 16 ç³»åˆ— (sm_75)
                is_supported = True
                gpu_series = "RTX 20/GTX 16 ç³»åˆ—"
                print(f"   âœ… {gpu_series} (sm_{props.major}{props.minor}) æª¢æ¸¬æˆåŠŸ!")
            elif props.major >= 6:  # GTX 10 ç³»åˆ—åŠä»¥ä¸Š (sm_60+)
                is_supported = True
                gpu_series = "GTX 10 ç³»åˆ—+"
                print(f"   âš ï¸ {gpu_series} (sm_{props.major}{props.minor}) åŸºæœ¬æ”¯æ´")
            else:
                print(f"   âŒ ä¸æ”¯æ´çš„æ¶æ§‹ (sm_{props.major}{props.minor})")
            
            if is_supported:
                rtx_series_found = True
                best_gpu_arch = max(best_gpu_arch, props.major * 10 + props.minor)
                print(f"   ğŸ“Š GPU ç³»åˆ—: {gpu_series}")
            
        except Exception as e:
            print(f"   âŒ GPU {i} ä¿¡æ¯ç²å–å¤±æ•—: {e}")
    
    if rtx_series_found:
        print(f"\nâœ… æª¢æ¸¬åˆ°æ”¯æ´çš„ GPUï¼Œæœ€é«˜æ¶æ§‹: sm_{best_gpu_arch // 10}{best_gpu_arch % 10}")
    else:
        print("\nâš ï¸ æœªæª¢æ¸¬åˆ°æ”¯æ´çš„ RTX/GTX ç³»åˆ— GPU")
    
    return rtx_series_found

def test_basic_cuda_operations():
    """æ¸¬è©¦åŸºæœ¬ CUDA æ“ä½œ"""
    print_section("åŸºæœ¬ CUDA æ“ä½œæ¸¬è©¦")
    
    if not torch.cuda.is_available():
        print("âŒ CUDA ä¸å¯ç”¨ï¼Œè·³éæ¸¬è©¦")
        return False
    
    try:
        # åŸºæœ¬å¼µé‡æ“ä½œ
        print("ğŸ§ª æ¸¬è©¦åŸºæœ¬å¼µé‡æ“ä½œ...")
        x = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0], device='cuda')
        y = x * 2
        z = torch.sum(y)
        result = z.cpu().item()
        print(f"   âœ… åŸºæœ¬é‹ç®—æˆåŠŸ: sum([1,2,3,4,5] * 2) = {result}")
        
        # çŸ©é™£ä¹˜æ³•
        print("ğŸ§ª æ¸¬è©¦çŸ©é™£ä¹˜æ³•...")
        a = torch.randn(128, 128, device='cuda')
        b = torch.randn(128, 128, device='cuda')
        c = torch.matmul(a, b)
        print(f"   âœ… çŸ©é™£ä¹˜æ³•æˆåŠŸ: {c.shape}")
        
        # å¤§å‹å¼µé‡æ“ä½œ
        print("ğŸ§ª æ¸¬è©¦å¤§å‹å¼µé‡æ“ä½œ...")
        large_tensor = torch.randn(1024, 1024, device='cuda')
        result = torch.sum(large_tensor)
        print(f"   âœ… å¤§å‹å¼µé‡æ“ä½œæˆåŠŸ: sum = {result.cpu().item():.2f}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ CUDA æ“ä½œå¤±æ•—: {e}")
        traceback.print_exc()
        return False

def test_mixed_precision():
    """æ¸¬è©¦æ··åˆç²¾åº¦"""
    print_section("æ··åˆç²¾åº¦æ¸¬è©¦")
    
    if not torch.cuda.is_available():
        print("âŒ CUDA ä¸å¯ç”¨ï¼Œè·³éæ¸¬è©¦")
        return False
    
    try:
        # è‡ªå‹•æ··åˆç²¾åº¦
        print("ğŸ§ª æ¸¬è©¦è‡ªå‹•æ··åˆç²¾åº¦ (AMP)...")
        
        with torch.cuda.amp.autocast():
            x = torch.randn(256, 256, device='cuda')
            y = torch.randn(256, 256, device='cuda')
            z = torch.matmul(x, y)
            result = torch.sum(z)
        
        print(f"   âœ… AMP æ¸¬è©¦æˆåŠŸ: {result.cpu().item():.2f}")
        
        # FP16 æ¸¬è©¦
        print("ğŸ§ª æ¸¬è©¦ FP16 æ“ä½œ...")
        x_fp16 = torch.randn(512, 512, dtype=torch.float16, device='cuda')
        y_fp16 = torch.randn(512, 512, dtype=torch.float16, device='cuda')
        z_fp16 = torch.matmul(x_fp16, y_fp16)
        print(f"   âœ… FP16 æ¸¬è©¦æˆåŠŸ: {z_fp16.shape}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ æ··åˆç²¾åº¦æ¸¬è©¦å¤±æ•—: {e}")
        traceback.print_exc()
        return False

def test_whisper_compatibility():
    """æ¸¬è©¦ Whisper å…¼å®¹æ€§"""
    print_section("Whisper å…¼å®¹æ€§æ¸¬è©¦")
    
    try:
        import whisper
        print("âœ… Whisper æ¨¡çµ„å°å…¥æˆåŠŸ")
        
        # æ¸¬è©¦æ¨¡å‹åˆ—è¡¨
        available_models = whisper.available_models()
        print(f"å¯ç”¨æ¨¡å‹: {available_models}")
        
        if torch.cuda.is_available():
            print("ğŸ§ª æ¸¬è©¦ GPU Whisper æ¨¡å‹è¼‰å…¥...")
            
            try:
                # å˜—è©¦è¼‰å…¥æœ€å°çš„æ¨¡å‹
                model = whisper.load_model("tiny", device="cuda")
                print("âœ… GPU Whisper æ¨¡å‹è¼‰å…¥æˆåŠŸ!")
                
                # æ¸¬è©¦ç°¡å–®æ¨ç†
                print("ğŸ§ª æ¸¬è©¦ GPU æ¨ç†...")
                dummy_audio = np.random.randn(16000).astype(np.float32)
                result = model.transcribe(dummy_audio, fp16=True, verbose=False)
                print(f"âœ… GPU æ¨ç†æˆåŠŸ: '{result['text'][:50]}...'")
                
                return True
                
            except Exception as gpu_error:
                print(f"âŒ GPU Whisper æ¸¬è©¦å¤±æ•—: {gpu_error}")
                print("å®Œæ•´éŒ¯èª¤ä¿¡æ¯:")
                traceback.print_exc()
                
                # å˜—è©¦ CPU æ¨¡å¼
                print("ğŸ§ª å›é€€åˆ° CPU æ¨¡å¼æ¸¬è©¦...")
                model = whisper.load_model("tiny", device="cpu")
                print("âœ… CPU Whisper æ¨¡å‹è¼‰å…¥æˆåŠŸ")
                return False
        else:
            print("âš ï¸ CUDA ä¸å¯ç”¨ï¼Œåƒ…æ¸¬è©¦ CPU æ¨¡å¼")
            model = whisper.load_model("tiny", device="cpu")
            print("âœ… CPU Whisper æ¨¡å‹è¼‰å…¥æˆåŠŸ")
            return False
            
    except ImportError:
        print("âŒ ç„¡æ³•å°å…¥ Whisper")
        return False
    except Exception as e:
        print(f"âŒ Whisper æ¸¬è©¦å¤±æ•—: {e}")
        traceback.print_exc()
        return False

def benchmark_gpu_performance():
    """GPU æ€§èƒ½åŸºæº–æ¸¬è©¦"""
    print_section("GPU æ€§èƒ½åŸºæº–æ¸¬è©¦")
    
    if not torch.cuda.is_available():
        print("âŒ CUDA ä¸å¯ç”¨ï¼Œè·³éæ€§èƒ½æ¸¬è©¦")
        return
    
    try:
        # çŸ©é™£ä¹˜æ³•æ€§èƒ½æ¸¬è©¦
        sizes = [512, 1024, 2048]
        
        for size in sizes:
            print(f"ğŸƒ æ¸¬è©¦ {size}x{size} çŸ©é™£ä¹˜æ³•æ€§èƒ½...")
            
            # é ç†±
            a = torch.randn(size, size, device='cuda')
            b = torch.randn(size, size, device='cuda')
            _ = torch.matmul(a, b)
            torch.cuda.synchronize()
            
            # æ€§èƒ½æ¸¬è©¦
            num_iterations = 10
            start_time = time.time()
            
            for _ in range(num_iterations):
                c = torch.matmul(a, b)
                torch.cuda.synchronize()
            
            end_time = time.time()
            avg_time = (end_time - start_time) / num_iterations
            gflops = (2 * size**3) / (avg_time * 1e9)
            
            print(f"   å¹³å‡æ™‚é–“: {avg_time*1000:.2f} ms")
            print(f"   æ€§èƒ½: {gflops:.2f} GFLOPS")
            
            # è¨˜æ†¶é«”ä½¿ç”¨
            memory_used = torch.cuda.memory_allocated() / 1024**3
            print(f"   GPU è¨˜æ†¶é«”ä½¿ç”¨: {memory_used:.2f} GB")
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½æ¸¬è©¦å¤±æ•—: {e}")
        traceback.print_exc()

def generate_report():
    """ç”Ÿæˆè¨ºæ–·å ±å‘Š"""
    print_header("RTX 50 ç³»åˆ— GPU è¨ºæ–·å ±å‘Š")
    
    results = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "basic_environment": True,
        "cuda_available": False,
        "rtx_series_detected": False,
        "basic_operations": False,
        "mixed_precision": False,
        "whisper_gpu_compatible": False,
        "overall_status": "æœªçŸ¥"
    }
    
    try:
        # åŸºæœ¬ç’°å¢ƒæª¢æŸ¥
        check_basic_environment()
        
        # CUDA å¯ç”¨æ€§
        results["cuda_available"] = check_cuda_availability()
        
        if results["cuda_available"]:
            # GPU è©³ç´°ä¿¡æ¯
            results["rtx_series_detected"] = check_gpu_details()
            
            # åŸºæœ¬æ“ä½œæ¸¬è©¦
            results["basic_operations"] = test_basic_cuda_operations()
            
            # æ··åˆç²¾åº¦æ¸¬è©¦
            results["mixed_precision"] = test_mixed_precision()
            
            # æ€§èƒ½åŸºæº–æ¸¬è©¦
            benchmark_gpu_performance()
        
        # Whisper å…¼å®¹æ€§æ¸¬è©¦
        results["whisper_gpu_compatible"] = test_whisper_compatibility()
        
        # ç¸½é«”ç‹€æ…‹è©•ä¼°
        if results["rtx_series_detected"] and results["basic_operations"] and results["whisper_gpu_compatible"]:
            results["overall_status"] = "âœ… å®Œå…¨å…¼å®¹"
        elif results["cuda_available"] and results["basic_operations"]:
            results["overall_status"] = "ğŸŸ¡ éƒ¨åˆ†å…¼å®¹"
        else:
            results["overall_status"] = "âŒ ä¸å…¼å®¹"
        
        # æ‰“å°ç¸½çµ
        print_header("è¨ºæ–·ç¸½çµ")
        print(f"æ™‚é–“æˆ³: {results['timestamp']}")
        print(f"CUDA å¯ç”¨: {'âœ…' if results['cuda_available'] else 'âŒ'}")
        print(f"RTX ç³»åˆ—æª¢æ¸¬: {'âœ…' if results['rtx_series_detected'] else 'âŒ'}")
        print(f"åŸºæœ¬ CUDA æ“ä½œ: {'âœ…' if results['basic_operations'] else 'âŒ'}")
        print(f"æ··åˆç²¾åº¦æ”¯æŒ: {'âœ…' if results['mixed_precision'] else 'âŒ'}")
        print(f"Whisper GPU å…¼å®¹: {'âœ…' if results['whisper_gpu_compatible'] else 'âŒ'}")
        print(f"ç¸½é«”ç‹€æ…‹: {results['overall_status']}")
        
        # å»ºè­°
        print_section("å»ºè­°")
        if results["overall_status"] == "âœ… å®Œå…¨å…¼å®¹":
            print("ğŸ‰ æ­å–œï¼æ‚¨çš„ RTX ç³»åˆ— GPU å·²å®Œå…¨æ”¯æŒ GPU åŠ é€Ÿ")
            print("ğŸ’¡ å»ºè­°ä½¿ç”¨æ··åˆç²¾åº¦ (fp16) ä»¥ç²å¾—æœ€ä½³æ€§èƒ½")
        elif results["overall_status"] == "ğŸŸ¡ éƒ¨åˆ†å…¼å®¹":
            print("âš ï¸ GPU å¯ç”¨ä½† Whisper å…¼å®¹æ€§æœ‰å•é¡Œ")
            print("ğŸ’¡ å»ºè­°æª¢æŸ¥ PyTorch ç‰ˆæœ¬æ˜¯å¦ç‚º nightly cu128")
        else:
            print("âŒ GPU åŠ é€Ÿä¸å¯ç”¨")
            print("ğŸ’¡ å»ºè­°ä½¿ç”¨ CPU å›é€€æ¨¡å¼")
        
        return results
        
    except Exception as e:
        print(f"âŒ è¨ºæ–·éç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {e}")
        traceback.print_exc()
        results["overall_status"] = "âŒ è¨ºæ–·å¤±æ•—"
        return results

def main():
    """ä¸»å‡½æ•¸"""
    results = generate_report()
    
    # ä¿å­˜çµæœåˆ°æ–‡ä»¶
    try:
        with open('/app/logs/gpu_diagnostics_report.json', 'w') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ“„ è¨ºæ–·å ±å‘Šå·²ä¿å­˜åˆ°: /app/logs/gpu_diagnostics_report.json")
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•ä¿å­˜å ±å‘Š: {e}")
    
    return results

if __name__ == "__main__":
    main()