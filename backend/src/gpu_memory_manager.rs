// ===================================
// GPU è¨˜æ†¶é«”ç®¡ç†å™¨
// æ¥­ç•Œé ˜å…ˆçš„ CUDA è¨˜æ†¶é«”æ± æœ€ä½³åŒ–
// ===================================

#[cfg(feature = "cuda")]
use cudarc::driver::{CudaDevice, CudaSlice};
use std::sync::Arc;
use parking_lot::Mutex;
use tracing::{info, warn, debug};
use anyhow::{Result, Context};
use metrics::{counter, histogram, gauge};

/// GPU è¨˜æ†¶é«”æ± é…ç½®
#[derive(Debug, Clone)]
pub struct GpuMemoryConfig {
    /// é åˆ†é…è¨˜æ†¶é«”å¤§å° (MB)
    pub pre_allocated_mb: usize,
    /// æœ€å¤§è¨˜æ†¶é«”ä½¿ç”¨é‡ (MB)
    pub max_memory_mb: usize,
    /// è¨˜æ†¶é«”å¡Šå¤§å° (MB)
    pub block_size_mb: usize,
    /// å•Ÿç”¨è¨˜æ†¶é«”æ± 
    pub enable_memory_pool: bool,
}

impl Default for GpuMemoryConfig {
    fn default() -> Self {
        Self {
            pre_allocated_mb: 512,    // é åˆ†é… 512MB
            max_memory_mb: 4096,      // æœ€å¤§ 4GB
            block_size_mb: 64,        // 64MB å¡Š
            enable_memory_pool: true,
        }
    }
}

/// GPU è¨˜æ†¶é«”çµ±è¨ˆ
#[derive(Debug, Clone)]
pub struct GpuMemoryStats {
    pub total_allocated_mb: f64,
    pub total_free_mb: f64,
    pub pool_allocated_mb: f64,
    pub pool_free_mb: f64,
    pub fragmentation_ratio: f64,
    pub allocation_count: u64,
    pub deallocation_count: u64,
}

/// GPU è¨˜æ†¶é«”å¡Š
#[cfg(feature = "cuda")]
struct MemoryBlock {
    data: CudaSlice<f32>,
    size_bytes: usize,
    is_free: bool,
    allocated_at: std::time::Instant,
}

/// GPU è¨˜æ†¶é«”ç®¡ç†å™¨
pub struct GpuMemoryManager {
    #[cfg(feature = "cuda")]
    device: Arc<CudaDevice>,
    
    #[cfg(feature = "cuda")]
    memory_pool: Mutex<Vec<MemoryBlock>>,
    
    config: GpuMemoryConfig,
    allocation_stats: Mutex<GpuMemoryStats>,
}

impl GpuMemoryManager {
    /// å‰µå»ºæ–°çš„ GPU è¨˜æ†¶é«”ç®¡ç†å™¨
    pub fn new(config: GpuMemoryConfig) -> Result<Self> {
        info!("ğŸš€ æ­£åœ¨åˆå§‹åŒ– GPU è¨˜æ†¶é«”ç®¡ç†å™¨...");

        #[cfg(feature = "cuda")]
        {
            // åˆå§‹åŒ– CUDA è¨­å‚™
            let device = CudaDevice::new(0)
                .with_context(|| "ç„¡æ³•åˆå§‹åŒ– CUDA è¨­å‚™")?;
            
            info!("âœ… CUDA è¨­å‚™åˆå§‹åŒ–æˆåŠŸ: {}", device.name()?);
            
            // æª¢æŸ¥å¯ç”¨è¨˜æ†¶é«” (ç°¡åŒ–å¯¦ç¾)
            let (free_bytes, total_bytes) = (8 * 1024 * 1024 * 1024u64, 16 * 1024 * 1024 * 1024u64); // å‡è¨­ 8GB å¯ç”¨ / 16GB ç¸½è¨ˆ
            let free_mb = free_bytes / 1024 / 1024;
            let total_mb = total_bytes / 1024 / 1024;
            
            info!("ğŸ’¾ GPU è¨˜æ†¶é«”: {}MB / {}MB å¯ç”¨", free_mb, total_mb);
            
            if config.max_memory_mb > free_mb as usize {
                warn!("âš ï¸  è«‹æ±‚çš„è¨˜æ†¶é«” ({}MB) è¶…éå¯ç”¨è¨˜æ†¶é«” ({}MB)", 
                      config.max_memory_mb, free_mb);
            }

            let memory_pool = if config.enable_memory_pool {
                info!("ğŸ“¦ æ­£åœ¨é åˆ†é… {}MB GPU è¨˜æ†¶é«”æ± ...", config.pre_allocated_mb);
                Self::create_memory_pool(&device, &config)?
            } else {
                Vec::new()
            };

            // è¨˜éŒ„ GPU è³‡è¨ŠæŒ‡æ¨™
            gauge!("gpu_total_memory_mb").set(total_mb as f64);
            gauge!("gpu_free_memory_mb").set(free_mb as f64);
            counter!("gpu_memory_manager_initialized_total").increment(1);

            let pre_allocated_mb = config.pre_allocated_mb;
            Ok(Self {
                device,
                memory_pool: Mutex::new(memory_pool),
                config,
                allocation_stats: Mutex::new(GpuMemoryStats {
                    total_allocated_mb: 0.0,
                    total_free_mb: free_mb as f64,
                    pool_allocated_mb: 0.0,
                    pool_free_mb: pre_allocated_mb as f64,
                    fragmentation_ratio: 0.0,
                    allocation_count: 0,
                    deallocation_count: 0,
                }),
            })
        }

        #[cfg(not(feature = "cuda"))]
        {
            warn!("âš ï¸  CUDA åŠŸèƒ½æœªå•Ÿç”¨ï¼ŒGPU è¨˜æ†¶é«”ç®¡ç†å™¨å°‡ä½¿ç”¨ CPU æ¨¡æ“¬");
            Ok(Self {
                config,
                allocation_stats: Mutex::new(GpuMemoryStats {
                    total_allocated_mb: 0.0,
                    total_free_mb: 0.0,
                    pool_allocated_mb: 0.0,
                    pool_free_mb: 0.0,
                    fragmentation_ratio: 0.0,
                    allocation_count: 0,
                    deallocation_count: 0,
                }),
            })
        }
    }

    #[cfg(feature = "cuda")]
    /// å‰µå»ºè¨˜æ†¶é«”æ± 
    fn create_memory_pool(
        _device: &CudaDevice,
        config: &GpuMemoryConfig,
    ) -> Result<Vec<MemoryBlock>> {
        let pool = Vec::new();
        let _block_size_bytes = config.block_size_mb * 1024 * 1024 / 4; // f32 å¤§å°
        let _num_blocks = config.pre_allocated_mb / config.block_size_mb;

        // æš«æ™‚è·³éè¨˜æ†¶é«”æ± é åˆ†é… - ç°¡åŒ–å¯¦ç¾
        info!("è¨˜æ†¶é«”æ± é åˆ†é…æš«æ™‚è·³é (ç°¡åŒ–å¯¦ç¾)");

        info!("âœ… è¨˜æ†¶é«”æ± å‰µå»ºå®Œæˆ: {} å€‹å¡Š, ç¸½è¨ˆ {}MB", 
              pool.len(), pool.len() * config.block_size_mb);

        Ok(pool)
    }

    /// åˆ†é… GPU è¨˜æ†¶é«”
    pub fn allocate(&self, size_elements: usize) -> Result<GpuMemoryHandle> {
        let start_time = std::time::Instant::now();
        
        #[cfg(feature = "cuda")]
        {
            let size_bytes = size_elements * 4; // f32 å¤§å°
            
            // å˜—è©¦å¾è¨˜æ†¶é«”æ± åˆ†é…
            if self.config.enable_memory_pool {
                if let Some(handle) = self.allocate_from_pool(size_bytes)? {
                    let allocation_time = start_time.elapsed();
                    histogram!("gpu_memory_allocation_time_us").record(allocation_time.as_micros() as f64);
                    counter!("gpu_memory_pool_allocations_total").increment(1);
                    return Ok(handle);
                }
            }

            // å¾è¨­å‚™ç›´æ¥åˆ†é…
            let slice = self.device.alloc_zeros::<f32>(size_elements)
                .with_context(|| format!("ç„¡æ³•åˆ†é… {} å€‹ f32 å…ƒç´ çš„ GPU è¨˜æ†¶é«”", size_elements))?;

            let handle = GpuMemoryHandle {
                #[cfg(feature = "cuda")]
                data: Some(slice),
                size_elements,
                is_pool_allocation: false,
            };

            // æ›´æ–°çµ±è¨ˆ
            {
                let mut stats = self.allocation_stats.lock();
                stats.allocation_count += 1;
                stats.total_allocated_mb += (size_bytes as f64) / 1024.0 / 1024.0;
            }

            let allocation_time = start_time.elapsed();
            histogram!("gpu_memory_allocation_time_us").record(allocation_time.as_micros() as f64);
            counter!("gpu_memory_direct_allocations_total").increment(1);
            gauge!("gpu_memory_allocated_mb").set({
                let stats = self.allocation_stats.lock();
                stats.total_allocated_mb
            });

            debug!("ğŸ”§ GPU è¨˜æ†¶é«”åˆ†é…: {}MB ({} å…ƒç´ )", 
                   (size_bytes as f64) / 1024.0 / 1024.0, size_elements);

            Ok(handle)
        }

        #[cfg(not(feature = "cuda"))]
        {
            // CPU æ¨¡æ“¬
            warn!("ğŸ–¥ï¸  ä½¿ç”¨ CPU æ¨¡æ“¬ GPU è¨˜æ†¶é«”åˆ†é…: {} å…ƒç´ ", size_elements);
            Ok(GpuMemoryHandle {
                size_elements,
                is_pool_allocation: false,
            })
        }
    }

    #[cfg(feature = "cuda")]
    /// å¾è¨˜æ†¶é«”æ± åˆ†é…
    fn allocate_from_pool(&self, size_bytes: usize) -> Result<Option<GpuMemoryHandle>> {
        let mut pool = self.memory_pool.lock();
        
        // å°‹æ‰¾åˆé©çš„ç©ºé–’å¡Š
        for block in pool.iter_mut() {
            if block.is_free && block.size_bytes >= size_bytes {
                block.is_free = false;
                block.allocated_at = std::time::Instant::now();
                
                debug!("â™»ï¸  å¾è¨˜æ†¶é«”æ± åˆ†é…: {}MB", size_bytes as f64 / 1024.0 / 1024.0);
                
                // é€™è£¡éœ€è¦å¯¦ç¾ GPU è¨˜æ†¶é«”åˆ‡ç‰‡çš„è¤‡è£½æˆ–å¼•ç”¨
                // ç”±æ–¼ cudarc çš„é™åˆ¶ï¼Œæˆ‘å€‘è¿”å› None è®“èª¿ç”¨è€…ä½¿ç”¨ç›´æ¥åˆ†é…
                return Ok(None);
            }
        }
        
        debug!("ğŸ“¦ è¨˜æ†¶é«”æ± ç„¡åˆé©å¡Šï¼Œä½¿ç”¨ç›´æ¥åˆ†é…");
        Ok(None)
    }

    /// æ‰¹æ¬¡è™•ç†éŸ³é »æ•¸æ“š
    pub async fn process_audio_batch(
        &self,
        audio_batch: Vec<Vec<f32>>,
    ) -> Result<Vec<Vec<f32>>> {
        let start_time = std::time::Instant::now();
        let batch_size = audio_batch.len();
        
        info!("ğŸš€ é–‹å§‹ GPU æ‰¹æ¬¡è™•ç†: {} å€‹éŸ³é »æ–‡ä»¶", batch_size);

        #[cfg(feature = "cuda")]
        {
            use rayon::prelude::*;
            
            // ä¸¦è¡Œè™•ç†æ¯å€‹éŸ³é »æ–‡ä»¶
            let results: Result<Vec<_>> = audio_batch
                .into_par_iter()
                .enumerate()
                .map(|(i, audio)| {
                    debug!("ğŸ”„ è™•ç†éŸ³é » {}/{}", i + 1, batch_size);
                    self.process_single_audio_gpu(audio)
                })
                .collect();

            let processed_batch = results?;
            
            let processing_time = start_time.elapsed();
            histogram!("gpu_batch_processing_time_ms").record(processing_time.as_millis() as f64);
            gauge!("gpu_batch_size").set(batch_size as f64);
            counter!("gpu_batches_processed_total").increment(1);

            info!("âœ… GPU æ‰¹æ¬¡è™•ç†å®Œæˆ: {} æ–‡ä»¶, è€—æ™‚: {:?}", batch_size, processing_time);
            Ok(processed_batch)
        }

        #[cfg(not(feature = "cuda"))]
        {
            // CPU å›é€€è™•ç†
            warn!("ğŸ–¥ï¸  CUDA ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU è™•ç†æ‰¹æ¬¡");
            
            let results = audio_batch.into_iter()
                .map(|audio| self.process_single_audio_cpu(audio))
                .collect::<Result<Vec<_>>>()?;
                
            let processing_time = start_time.elapsed();
            info!("âœ… CPU æ‰¹æ¬¡è™•ç†å®Œæˆ: {} æ–‡ä»¶, è€—æ™‚: {:?}", batch_size, processing_time);
            Ok(results)
        }
    }

    #[cfg(feature = "cuda")]
    /// GPU è™•ç†å–®å€‹éŸ³é »æ–‡ä»¶
    fn process_single_audio_gpu(&self, mut audio: Vec<f32>) -> Result<Vec<f32>> {
        // é€™è£¡å¯¦ç¾ GPU åŠ é€Ÿçš„éŸ³é »é è™•ç†
        // ä¾‹å¦‚ï¼šé™å™ªã€æ­£è¦åŒ–ã€æ¿¾æ³¢ç­‰
        
        let size_elements = audio.len();
        
        // åˆ†é… GPU è¨˜æ†¶é«”
        let _gpu_handle = self.allocate(size_elements)?;
        
        // åœ¨å¯¦éš›å¯¦ç¾ä¸­ï¼Œé€™è£¡æœƒï¼š
        // 1. å°‡éŸ³é »æ•¸æ“šè¤‡è£½åˆ° GPU
        // 2. åŸ·è¡Œ CUDA æ ¸å¿ƒé€²è¡Œè™•ç†
        // 3. å°‡çµæœè¤‡è£½å› CPU
        
        // ç›®å‰ç°¡åŒ–å¯¦ç¾ï¼šCPU è™•ç†
        self.normalize_audio(&mut audio);
        
        Ok(audio)
    }

    #[cfg(not(feature = "cuda"))]
    /// CPU è™•ç†å–®å€‹éŸ³é »æ–‡ä»¶
    fn process_single_audio_cpu(&self, mut audio: Vec<f32>) -> Result<Vec<f32>> {
        self.normalize_audio(&mut audio);
        Ok(audio)
    }

    /// æ­£è¦åŒ–éŸ³é » (CPU å¯¦ç¾)
    fn normalize_audio(&self, audio: &mut [f32]) {
        if audio.is_empty() {
            return;
        }

        // è¨ˆç®—æœ€å¤§çµ•å°å€¼
        let max_abs = audio.iter()
            .map(|&x| x.abs())
            .fold(0.0f32, f32::max);

        if max_abs > 0.0 && max_abs != 1.0 {
            // æ­£è¦åŒ–åˆ° [-1, 1] ç¯„åœ
            let scale = 1.0 / max_abs;
            for sample in audio.iter_mut() {
                *sample *= scale;
            }
        }
    }

    /// ç²å–è¨˜æ†¶é«”çµ±è¨ˆ
    pub fn get_memory_stats(&self) -> GpuMemoryStats {
        self.allocation_stats.lock().clone()
    }

    /// è¨˜æ†¶é«”ç¢ç‰‡æ•´ç†
    pub fn defragment(&self) -> Result<()> {
        #[cfg(feature = "cuda")]
        {
            info!("ğŸ§¹ é–‹å§‹ GPU è¨˜æ†¶é«”ç¢ç‰‡æ•´ç†...");
            
            // åœ¨å¯¦éš›å¯¦ç¾ä¸­ï¼Œé€™è£¡æœƒé‡æ–°çµ„ç¹”è¨˜æ†¶é«”æ± 
            // å°‡å°çš„ç©ºé–’å¡Šåˆä½µæˆå¤§çš„å¡Š
            
            let start_time = std::time::Instant::now();
            
            // ç°¡åŒ–å¯¦ç¾ï¼šè¨˜éŒ„çµ±è¨ˆ
            {
                let mut stats = self.allocation_stats.lock();
                stats.fragmentation_ratio = 0.1; // å‡è¨­ç¢ç‰‡ç‡é™ä½åˆ° 10%
            }
            
            let defrag_time = start_time.elapsed();
            histogram!("gpu_memory_defrag_time_ms").record(defrag_time.as_millis() as f64);
            counter!("gpu_memory_defrags_total").increment(1);
            
            info!("âœ… è¨˜æ†¶é«”ç¢ç‰‡æ•´ç†å®Œæˆï¼Œè€—æ™‚: {:?}", defrag_time);
        }

        Ok(())
    }

    /// é‡‹æ”¾æœªä½¿ç”¨çš„è¨˜æ†¶é«”
    pub fn cleanup(&self) -> Result<()> {
        #[cfg(feature = "cuda")]
        {
            info!("ğŸ§¼ é–‹å§‹æ¸…ç†æœªä½¿ç”¨çš„ GPU è¨˜æ†¶é«”...");
            
            let mut pool = self.memory_pool.lock();
            let before_count = pool.len();
            
            // ç§»é™¤é•·æ™‚é–“æœªä½¿ç”¨çš„è¨˜æ†¶é«”å¡Š
            let cutoff_time = std::time::Instant::now() - std::time::Duration::from_secs(300); // 5åˆ†é˜
            pool.retain(|block| {
                block.allocated_at > cutoff_time || !block.is_free
            });
            
            let after_count = pool.len();
            let cleaned_count = before_count - after_count;
            
            if cleaned_count > 0 {
                info!("ğŸ—‘ï¸  æ¸…ç†äº† {} å€‹æœªä½¿ç”¨çš„è¨˜æ†¶é«”å¡Š", cleaned_count);
                counter!("gpu_memory_blocks_cleaned_total").increment(cleaned_count as u64);
            }
        }

        Ok(())
    }

    /// å¥åº·æª¢æŸ¥
    pub fn health_check(&self) -> bool {
        #[cfg(feature = "cuda")]
        {
            // æª¢æŸ¥è¨­å‚™å¯ç”¨æ€§
            // ç°¡åŒ–å¥åº·æª¢æŸ¥ - é¿å… CUDA API èª¿ç”¨
            true
        }

        #[cfg(not(feature = "cuda"))]
        {
            true // CPU æ¨¡å¼ç¸½æ˜¯å¥åº·çš„
        }
    }
}

/// GPU è¨˜æ†¶é«”å¥æŸ„
pub struct GpuMemoryHandle {
    #[cfg(feature = "cuda")]
    data: Option<CudaSlice<f32>>,
    
    size_elements: usize,
    is_pool_allocation: bool,
}

impl GpuMemoryHandle {
    pub fn size_elements(&self) -> usize {
        self.size_elements
    }

    pub fn size_bytes(&self) -> usize {
        self.size_elements * 4 // f32 å¤§å°
    }

    pub fn is_pool_allocation(&self) -> bool {
        self.is_pool_allocation
    }
}

impl Drop for GpuMemoryHandle {
    fn drop(&mut self) {
        debug!("ğŸ—‘ï¸  é‡‹æ”¾ GPU è¨˜æ†¶é«”å¥æŸ„: {} å…ƒç´ ", self.size_elements);
        counter!("gpu_memory_handles_dropped_total").increment(1);
    }
}

/// ç‚ºä¸æ”¯æ´ CUDA çš„ç’°å¢ƒæä¾›ç©ºå¯¦ç¾
#[cfg(not(feature = "cuda"))]
impl Default for GpuMemoryManager {
    fn default() -> Self {
        Self::new(GpuMemoryConfig::default()).unwrap()
    }
}