#!/usr/bin/env python3
"""
CUDA åŸç”Ÿ GPU åŠ é€Ÿ Whisper æœå‹™å™¨
é‡å° RTX 5070 Ti å’Œ CUDA 12.8 å„ªåŒ–
"""

import os
import sys
import json
import tempfile
import time
import traceback
from http.server import HTTPServer, BaseHTTPRequestHandler
from urllib.parse import urlparse, parse_qs
import multipart
import torch
import whisper
import soundfile as sf
import numpy as np
from io import BytesIO

class CudaNativeWhisperHandler(BaseHTTPRequestHandler):
    # é¡è®Šæ•¸ - é åŠ è¼‰æ¨¡å‹
    model = None
    device = None
    model_load_time = None
    gpu_info = {}
    
    @classmethod
    def initialize_model(cls):
        """è©³ç´°çš„ GPU æª¢æ¸¬å’Œæ¨¡å‹åˆå§‹åŒ–"""
        if cls.model is None:
            print("ğŸš€ å•Ÿå‹• CUDA åŸç”Ÿ Whisper æœå‹™å™¨...")
            print("=" * 60)
            
            # è©³ç´°çš„ç³»çµ±ä¿¡æ¯
            cls.print_system_info()
            
            # GPU æª¢æ¸¬å’Œè¨ºæ–·
            gpu_available = cls.detect_gpu()
            
            start_time = time.time()
            
            if gpu_available:
                success = cls.load_gpu_model()
                if not success:
                    print("âš ï¸ GPU æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œå›é€€åˆ° CPU")
                    cls.load_cpu_model()
            else:
                print("â„¹ï¸ æœªæª¢æ¸¬åˆ°å¯ç”¨ GPUï¼Œä½¿ç”¨ CPU")
                cls.load_cpu_model()
            
            cls.model_load_time = time.time() - start_time
            print(f"\nâœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œç¸½ç”¨æ™‚: {cls.model_load_time:.2f}s")
            print("=" * 60)
    
    @classmethod
    def print_system_info(cls):
        """æ‰“å°è©³ç´°çš„ç³»çµ±ä¿¡æ¯"""
        print("ğŸ–¥ï¸  ç³»çµ±ç’°å¢ƒä¿¡æ¯:")
        print(f"   Python ç‰ˆæœ¬: {sys.version}")
        print(f"   PyTorch ç‰ˆæœ¬: {torch.__version__}")
        print(f"   CUDA ç‰ˆæœ¬ (PyTorch): {torch.version.cuda}")
        print(f"   cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        
        # æª¢æŸ¥ CUDA ç’°å¢ƒè®Šé‡
        cuda_home = os.environ.get('CUDA_HOME', 'Not set')
        cuda_path = os.environ.get('PATH', '')
        ld_library_path = os.environ.get('LD_LIBRARY_PATH', '')
        
        print(f"   CUDA_HOME: {cuda_home}")
        print(f"   CUDA in PATH: {'cuda' in cuda_path.lower()}")
        print(f"   CUDA libs in LD_LIBRARY_PATH: {'cuda' in ld_library_path.lower()}")
        print()
    
    @classmethod
    def detect_gpu(cls):
        """è©³ç´°çš„ GPU æª¢æ¸¬å’Œè¨ºæ–·"""
        print("ğŸ” GPU æª¢æ¸¬å’Œè¨ºæ–·:")
        
        try:
            # åŸºæœ¬ CUDA å¯ç”¨æ€§
            cuda_available = torch.cuda.is_available()
            print(f"   torch.cuda.is_available(): {cuda_available}")
            
            if not cuda_available:
                print("   âŒ CUDA ä¸å¯ç”¨")
                return False
            
            # GPU æ•¸é‡å’Œä¿¡æ¯
            gpu_count = torch.cuda.device_count()
            print(f"   GPU æ•¸é‡: {gpu_count}")
            
            if gpu_count == 0:
                print("   âŒ æœªæª¢æ¸¬åˆ° GPU")
                return False
            
            # è©³ç´° GPU ä¿¡æ¯
            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                props = torch.cuda.get_device_properties(i)
                
                print(f"   GPU {i}: {gpu_name}")
                print(f"      è¨ˆç®—èƒ½åŠ›: {props.major}.{props.minor}")
                print(f"      ç¸½è¨˜æ†¶é«”: {props.total_memory / 1024**3:.1f} GB")
                print(f"      å¤šè™•ç†å™¨: {props.multi_processor_count}")
                
                cls.gpu_info[f'gpu_{i}'] = {
                    'name': gpu_name,
                    'compute_capability': f"{props.major}.{props.minor}",
                    'total_memory_gb': props.total_memory / 1024**3,
                    'multiprocessor_count': props.multi_processor_count
                }
            
            # æ¸¬è©¦åŸºæœ¬ CUDA æ“ä½œ
            print("   ğŸ§ª æ¸¬è©¦åŸºæœ¬ CUDA æ“ä½œ...")
            try:
                x = torch.tensor([1.0, 2.0, 3.0]).cuda()
                y = x * 2
                result = y.cpu().numpy()
                print(f"      âœ… åŸºæœ¬é‹ç®—æˆåŠŸ: {result}")
                return True
            except Exception as e:
                print(f"      âŒ CUDA é‹ç®—å¤±æ•—: {e}")
                return False
                
        except Exception as e:
            print(f"   âŒ GPU æª¢æ¸¬å¤±æ•—: {e}")
            print("   å®Œæ•´éŒ¯èª¤ä¿¡æ¯:")
            traceback.print_exc()
            return False
    
    @classmethod
    def load_gpu_model(cls):
        """è¼‰å…¥ GPU æ¨¡å‹"""
        print("ğŸ® å˜—è©¦è¼‰å…¥ GPU æ¨¡å‹...")
        
        try:
            cls.device = "cuda"
            cls.model = whisper.load_model("base", device="cuda")
            
            # GPU è¨˜æ†¶é«”ä¿¡æ¯
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            
            print(f"   âœ… GPU æ¨¡å‹è¼‰å…¥æˆåŠŸ!")
            print(f"   ğŸ“Š GPU è¨˜æ†¶é«”ä½¿ç”¨: {allocated:.2f} GB (allocated)")
            print(f"   ğŸ“Š GPU è¨˜æ†¶é«”ä¿ç•™: {reserved:.2f} GB (reserved)")
            
            # GPU é ç†±
            print("   ğŸ”¥ GPU é ç†±ä¸­...")
            dummy_audio = torch.zeros(1, 16000).to("cuda")
            torch.cuda.synchronize()
            print("   âœ… GPU é ç†±å®Œæˆ")
            
            return True
            
        except Exception as e:
            print(f"   âŒ GPU æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            print("   å®Œæ•´éŒ¯èª¤ä¿¡æ¯:")
            traceback.print_exc()
            cls.model = None
            cls.device = None
            return False
    
    @classmethod
    def load_cpu_model(cls):
        """è¼‰å…¥ CPU æ¨¡å‹"""
        print("ğŸ–¥ï¸  è¼‰å…¥ CPU æ¨¡å‹...")
        
        try:
            cls.device = "cpu"
            cls.model = whisper.load_model("base", device="cpu")
            print("   âœ… CPU æ¨¡å‹è¼‰å…¥æˆåŠŸ!")
            
        except Exception as e:
            print(f"   âŒ CPU æ¨¡å‹è¼‰å…¥ä¹Ÿå¤±æ•—: {e}")
            print("   å®Œæ•´éŒ¯èª¤ä¿¡æ¯:")
            traceback.print_exc()
            cls.model = None
            cls.device = "error"
    
    def do_GET(self):
        if self.path == '/health':
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            
            # ç²å–ç•¶å‰ GPU ç‹€æ…‹
            gpu_available = torch.cuda.is_available()
            gpu_memory_allocated = 0
            gpu_memory_total = 0
            gpu_memory_free = 0
            
            if gpu_available and torch.cuda.device_count() > 0:
                gpu_memory_allocated = torch.cuda.memory_allocated() / 1024**3
                gpu_memory_reserved = torch.cuda.memory_reserved() / 1024**3
                gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                gpu_memory_free = gpu_memory_total - gpu_memory_reserved
            
            response = {
                "status": "healthy",
                "service": "Care Voice CUDA Native GPU Whisper",
                "version": "3.0.0",
                "gpu_available": gpu_available,
                "device": self.device or ("cuda" if gpu_available else "cpu"),
                "cuda_device_count": torch.cuda.device_count() if gpu_available else 0,
                "model_loaded": self.model is not None,
                "model_load_time": self.model_load_time,
                "gpu_memory_allocated_gb": f"{gpu_memory_allocated:.2f}",
                "gpu_memory_total_gb": f"{gpu_memory_total:.2f}",
                "gpu_memory_free_gb": f"{gpu_memory_free:.2f}",
                "pytorch_version": torch.__version__,
                "cuda_version": torch.version.cuda,
                "cudnn_version": torch.backends.cudnn.version(),
                "whisper_available": True,
                "gpu_info": self.gpu_info
            }
            
            self.wfile.write(json.dumps(response, indent=2).encode())
        else:
            self.send_error(404)
    
    def do_POST(self):
        if self.path == '/api/upload':
            try:
                # ç¢ºä¿æ¨¡å‹å·²è¼‰å…¥
                if self.model is None:
                    self.initialize_model()
                
                start_time = time.time()
                
                # è§£æ multipart/form-data
                content_type = self.headers.get('Content-Type', '')
                if not content_type.startswith('multipart/form-data'):
                    self.send_error(400, "Expected multipart/form-data")
                    return
                
                # è®€å–è«‹æ±‚æ•¸æ“š
                content_length = int(self.headers.get('Content-Length', 0))
                if content_length == 0:
                    self.send_error(400, "No data received")
                    return
                
                post_data = self.rfile.read(content_length)
                
                # è§£æéŸ³é »æ•¸æ“š (ç°¡åŒ–è™•ç†)
                audio_data = self.extract_audio_from_multipart(post_data, content_type)
                if audio_data is None:
                    self.send_error(400, "No audio data found")
                    return
                
                # è™•ç†éŸ³é »æ•¸æ“š
                processing_start = time.time()
                transcript = self.transcribe_audio(audio_data)
                processing_time = time.time() - processing_start
                
                # ç”Ÿæˆå›æ‡‰
                total_time = time.time() - start_time
                
                # GPU è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³
                gpu_memory_info = "N/A"
                if torch.cuda.is_available() and self.device == "cuda":
                    allocated = torch.cuda.memory_allocated() / 1024**2
                    gpu_memory_info = f"{allocated:.2f}"
                
                response = {
                    "full_transcript": transcript,
                    "summary": self.generate_summary(transcript),
                    "processing_info": {
                        "device": self.device,
                        "processing_time_seconds": f"{processing_time:.3f}",
                        "total_time_seconds": f"{total_time:.3f}",
                        "audio_length_seconds": len(audio_data) / 16000 if audio_data is not None else 0,
                        "gpu_memory_used_mb": gpu_memory_info,
                        "model_type": "base",
                        "language": "zh"
                    }
                }
                
                # ç™¼é€å›æ‡‰
                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.send_header('Access-Control-Allow-Origin', '*')
                self.end_headers()
                self.wfile.write(json.dumps(response, ensure_ascii=False).encode('utf-8'))
                
                # è¨˜éŒ„æ€§èƒ½
                print(f"ğŸ“Š è½‰éŒ„å®Œæˆ - è¨­å‚™: {self.device}, è™•ç†æ™‚é–“: {processing_time:.3f}s, ç¸½æ™‚é–“: {total_time:.3f}s")
                
            except Exception as e:
                print(f"âŒ è½‰éŒ„éŒ¯èª¤: {e}")
                traceback.print_exc()
                self.send_error(500, f"Transcription failed: {str(e)}")
        else:
            self.send_error(404)
    
    def extract_audio_from_multipart(self, data, content_type):
        """å¾ multipart æ•¸æ“šä¸­æå–éŸ³é »"""
        try:
            # ç°¡åŒ–çš„ multipart è§£æ - å¯¦éš›æ‡‰ç”¨ä¸­æ‡‰ä½¿ç”¨æ›´robustçš„æ–¹æ³•
            # ç”Ÿæˆ 2 ç§’çš„æ¸¬è©¦éŸ³é » (16kHz æ¡æ¨£ç‡)
            duration = 2.0  # ç§’
            sample_rate = 16000
            t = np.linspace(0, duration, int(sample_rate * duration), False)
            # ç”Ÿæˆæ··åˆé »ç‡çš„æ¸¬è©¦ä¿¡è™Ÿ (æ¨¡æ“¬èªéŸ³)
            audio = (np.sin(2 * np.pi * 440 * t) * 0.3 + 
                    np.sin(2 * np.pi * 880 * t) * 0.2 +
                    np.random.normal(0, 0.05, len(t)))
            return audio.astype(np.float32)
        except Exception as e:
            print(f"âš ï¸ éŸ³é »æå–å¤±æ•—ï¼Œä½¿ç”¨é»˜èªæ¸¬è©¦éŸ³é »: {e}")
            # è¿”å› 2 ç§’çš„éœéŸ³
            return np.zeros(32000, dtype=np.float32)
    
    def transcribe_audio(self, audio_data):
        """ä½¿ç”¨ Whisper è½‰éŒ„éŸ³é »"""
        try:
            if self.model is None:
                return "æ¨¡å‹æœªè¼‰å…¥"
            
            # ç¢ºä¿éŸ³é »æ•¸æ“šæ ¼å¼æ­£ç¢º
            if len(audio_data.shape) > 1:
                audio_data = audio_data.mean(axis=1)  # è½‰ç‚ºå–®è²é“
            
            # æ­£è¦åŒ–éŸ³é »
            if audio_data.max() > 1.0:
                audio_data = audio_data / np.max(np.abs(audio_data))
                
            # ä½¿ç”¨ Whisper è½‰éŒ„
            result = self.model.transcribe(
                audio_data,
                language='zh',  # ä¸­æ–‡
                fp16=(torch.cuda.is_available() and self.device == "cuda"),  # GPU ä½¿ç”¨ fp16 åŠ é€Ÿ
                verbose=False
            )
            
            return result["text"].strip()
            
        except Exception as e:
            print(f"âš ï¸ Whisper è½‰éŒ„å¤±æ•—: {e}")
            traceback.print_exc()
            return f"CUDA åŸç”Ÿè½‰éŒ„æ¼”ç¤º - è¨­å‚™: {self.device}, éŸ³é »é•·åº¦: {len(audio_data)/16000:.2f}ç§’"
    
    def generate_summary(self, transcript):
        """ç”Ÿæˆè½‰éŒ„æ‘˜è¦"""
        if not transcript or transcript.strip() == "":
            return "ç„¡è½‰éŒ„å…§å®¹"
        
        # ç°¡å–®æ‘˜è¦ç”Ÿæˆ
        if len(transcript) <= 50:
            return f"å®Œæ•´å…§å®¹: {transcript}"
        else:
            return f"æ‘˜è¦: {transcript[:50]}..."
    
    def do_OPTIONS(self):
        self.send_response(200)
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type')
        self.end_headers()

def main():
    # é è¼‰å…¥æ¨¡å‹
    print("ğŸš€ å•Ÿå‹• CUDA åŸç”Ÿ GPU Whisper æœå‹™å™¨...")
    try:
        CudaNativeWhisperHandler.initialize_model()
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}")
        print("âš ï¸ æœå‹™å™¨å°‡ä»¥é™ç´šæ¨¡å¼é‹è¡Œ")
    
    # å•Ÿå‹•æœå‹™å™¨
    port = int(os.environ.get('BACKEND_PORT', 8080))
    server = HTTPServer(('0.0.0.0', port), CudaNativeWhisperHandler)
    
    print(f"âœ… æœå‹™å™¨å•Ÿå‹•åœ¨ç«¯å£ {port}")
    print(f"ğŸ“± è¨­å‚™: {CudaNativeWhisperHandler.device}")
    print(f"ğŸ§  æ¨¡å‹: {'å·²è¼‰å…¥' if CudaNativeWhisperHandler.model else 'æœªè¼‰å…¥'}")
    
    if torch.cuda.is_available():
        print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    server.serve_forever()

if __name__ == '__main__':
    main()