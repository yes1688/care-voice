# Care Voice RTX 50 ç³»åˆ—æ§‹å»ºå’Œéƒ¨ç½²æŒ‡å—

## ğŸš€ RTX 50 ç³»åˆ—å¿«é€Ÿé–‹å§‹

### RTX 50 ç³»åˆ—é€šç”¨å®¹å™¨ (æ¨è–¦ï¼Œæ”¯æ´å¤šä¸–ä»£ GPU)

```bash
# æ§‹å»º RTX 50 ç³»åˆ—é€šç”¨å®¹å™¨
podman build -t care-voice-rtx50:latest -f Dockerfile.rtx50-series .

# é‹è¡Œ RTX 50 ç³»åˆ—å®¹å™¨ (æ”¯æ´ RTX 50/40/30/20 + GTX 10 ç³»åˆ—)
# ä½¿ç”¨ CDI (Container Device Interface) GPU å­˜å–
podman run -d --name care-voice-rtx50 \
    --device nvidia.com/gpu=all \
    -p 8001:8001 \
    -e NVIDIA_VISIBLE_DEVICES=all \
    -e CUDA_VISIBLE_DEVICES=0 \
    -e TORCH_CUDA_ARCH_LIST="6.0;6.1;7.0;7.5;8.0;8.6;8.9;12.0" \
    -e ENABLE_FP16=1 \
    care-voice-rtx50:latest

# æª¢æŸ¥ RTX 50 ç³»åˆ— GPU ç‹€æ…‹
podman exec care-voice-rtx50 nvidia-smi
podman exec care-voice-rtx50 python3 /app/gpu_diagnostics_rtx50.py
```

### èˆŠç‰ˆæœ¬å®¹å™¨ (å‘ä¸‹å…¼å®¹)

```bash
# èˆŠ GPU ç‰ˆæœ¬ (ä»å¯ä½¿ç”¨ï¼Œä½†ä¸æ”¯æ´ RTX 50 ç³»åˆ—)
podman build -t care-voice-legacy:latest -f legacy/Dockerfile.blackdx_gpu .

# èˆŠ CPU ç‰ˆæœ¬
podman build -t care-voice-cpu:latest -f legacy/Dockerfile.blackdx_cpu .
```

## ğŸ”§ RTX 50 ç³»åˆ—åŠŸèƒ½é©—è­‰

### 1. RTX 50 ç³»åˆ—å¥åº·æª¢æŸ¥
```bash
curl http://localhost:8001/health
# é æœŸå›æ‡‰: RTX 50 ç³»åˆ— GPU æª¢æ¸¬å’Œæœå‹™ç‹€æ…‹è³‡è¨Š
```

### 2. GPU è¨ºæ–·å…¨é¢æª¢æŸ¥
```bash
# é‹è¡Œå®Œæ•´ GPU è¨ºæ–·
podman exec care-voice-rtx50 python3 /app/gpu_diagnostics_rtx50.py

# æŸ¥çœ‹è¨ºæ–·å ±å‘Š
podman exec care-voice-rtx50 cat /app/logs/gpu_diagnostics_report.json
```

### 3. å‰ç«¯ä»‹é¢æ¸¬è©¦
æ‰“é–‹ç€è¦½å™¨è¨ªå•: http://localhost:8001

### 4. Whisper è½‰éŒ„æ¸¬è©¦
```bash
# ä¸Šå‚³éŸ³é »æ–‡ä»¶æ¸¬è©¦ (ä¸åŒæ¨¡å‹å¤§å°)
curl -X POST -F "audio=@test.wav" -F "model=tiny" http://localhost:8001/api/transcribe
curl -X POST -F "audio=@test.wav" -F "model=base" http://localhost:8001/api/transcribe
curl -X POST -F "audio=@test.wav" -F "model=large-v3" http://localhost:8001/api/transcribe

# æ··åˆç²¾åº¦æ¸¬è©¦
curl -X POST -F "precision=fp16" -F "audio=@test.wav" http://localhost:8001/api/transcribe
```

## ğŸ“Š RTX 50 ç³»åˆ—æ€§èƒ½å°æ¯”

### å¤šä¸–ä»£ GPU æ€§èƒ½å°æ¯”

| GPU ä¸–ä»£ | æ¶æ§‹ | 10ç§’éŸ³é »è½‰éŒ„æ™‚é–“ | ç›¸å° CPU æå‡ | FP16 é¡å¤–åŠ é€Ÿ |
|----------|------|------------------|-------------|----------------|
| RTX 50 ç³»åˆ— | sm_120 | ~0.2-0.4ç§’ | 20-30x | 2.5-3x |
| RTX 40 ç³»åˆ— | sm_89 | ~0.3-0.5ç§’ | 15-25x | 2.2-2.8x |
| RTX 30 ç³»åˆ— | sm_86 | ~0.4-0.7ç§’ | 10-18x | 1.8-2.2x |
| RTX 20 ç³»åˆ— | sm_75 | ~0.6-1.0ç§’ | 8-12x | 1.6-2.0x |
| GTX 10 ç³»åˆ— | sm_60+ | ~1.0-2.0ç§’ | 4-8x | 1.4-1.8x |
| CPU (8æ ¸) | - | ~5-8ç§’ | 1x | N/A |

### æ··åˆç²¾åº¦ VRAM ä½¿ç”¨æ•ˆç‡

- **FP32 æ¨¡å¼**: 4-8GB VRAM (åŸºæº–)
- **FP16 æ¨¡å¼**: 2-4GB VRAM (40-50% ç¯€çœ)
- **AMP æ¨¡å¼**: 2.2-4.4GB VRAM (35-45% ç¯€çœ)

## ğŸ› ï¸ RTX 50 ç³»åˆ—æ•…éšœæ’é™¤

### RTX 50 ç³»åˆ—ç‰¹å®šå•é¡Œ

#### 1. RTX 50 ç³»åˆ— sm_120 æ¶æ§‹ä¸è¢«è­˜åˆ¥
```bash
# æª¢æŸ¥ PyTorch æ˜¯å¦æ”¯æ´ sm_120
python3 -c "import torch; print(torch.cuda.get_arch_list())"
# æ‡‰è©²åŒ…å« '12.0' æˆ– 'sm_120'

# å¦‚æœä¸æ”¯æ´ï¼Œç¢ºèªä½¿ç”¨ PyTorch nightly cu128
podman exec care-voice-rtx50 pip show torch
```

#### 2. CUDA 12.8 ç›¸å®¹æ€§å•é¡Œ
```bash
# æª¢æŸ¥ä¸»æ©Ÿå’Œå®¹å™¨ CUDA ç‰ˆæœ¬åŒ¹é…
nvidia-smi  # ä¸»æ©Ÿ CUDA ç‰ˆæœ¬
podman exec care-voice-rtx50 nvcc --version  # å®¹å™¨ CUDA ç‰ˆæœ¬

# ç¢ºä¿é©…å‹•ç‰ˆæœ¬ >= 570.x (æ”¯æ´ CUDA 12.8)
```

#### 3. é–‹æºé©…å‹•ç›¸å®¹æ€§
```bash
# æª¢æŸ¥é©…å‹•é¡å‹
cat /proc/driver/nvidia/version
lsmod | grep nvidia

# å¦‚æœä½¿ç”¨é–‹æºé©…å‹•ï¼Œç¢ºä¿ç‰ˆæœ¬è¶³å¤ æ–°
sudo apt update && sudo apt upgrade nvidia-driver-570-open

# å®‰è£ NVIDIA Container Toolkit (å¿…éœ€ CDI æ”¯æ´)
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
echo "deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://nvidia.github.io/libnvidia-container/stable/deb/\$(ARCH) /" | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
sudo apt update && sudo apt install -y nvidia-container-toolkit=1.17.8-1

# ç”Ÿæˆ CDI æ“ä½œè¦ç¯„
nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml
nvidia-ctk cdi list  # ç¢ºèª CDI è£ç½®å¯ç”¨
```

#### 4. æ··åˆç²¾åº¦å•é¡Œ
```bash
# æ¸¬è©¦ FP16 æ”¯æ´
python3 -c "import torch; print('FP16:', torch.cuda.is_available() and torch.cuda.is_bf16_supported())"

# å¦‚æœ FP16 ä¸æ”¯æ´ï¼Œåœç”¨æ··åˆç²¾åº¦
export ENABLE_FP16=0
```

### RTX 50 ç³»åˆ—å®¹å™¨å•é¡Œ

#### 1. RTX 50 ç³»åˆ—æœå‹™å•Ÿå‹•æª¢æŸ¥
```bash
# æª¢æŸ¥æ‰€æœ‰æœå‹™ç‹€æ…‹
podman exec care-voice-rtx50 supervisorctl status

# æª¢æŸ¥ RTX 50 Whisper æœå‹™
podman exec care-voice-rtx50 ps aux | grep rtx50
```

#### 2. æŸ¥çœ‹ RTX 50 ç³»åˆ—è©³ç´°æ—¥èªŒ
```bash
# RTX 50 ä¸»æœå‹™æ—¥èªŒ
podman logs care-voice-rtx50

# RTX 50 Whisper æœå‹™æ—¥èªŒ
podman exec care-voice-rtx50 cat /app/logs/rtx50_whisper_service.log

# GPU è¨ºæ–·æ—¥èªŒ
podman exec care-voice-rtx50 cat /app/logs/gpu_diagnostics_report.json

# Supervisor ç³»çµ±æ—¥èªŒ
podman exec care-voice-rtx50 cat /app/logs/supervisor/supervisord.log
```

#### 3. GPU æ€§èƒ½å•é¡Œè¨ºæ–·
```bash
# å¯¦æ™‚ GPU ç›£æ§
podman exec care-voice-rtx50 watch -n 1 nvidia-smi

# æª¢æŸ¥ GPU è¨˜æ†¶é«”ä½¿ç”¨
podman exec care-voice-rtx50 python3 -c "import torch; print(f'VRAM: {torch.cuda.memory_allocated()/1024**3:.2f}GB')"
```

## ğŸ”„ å¾èˆŠç‰ˆæœ¬å‡ç´šåˆ° RTX 50 ç³»åˆ—

å¦‚æœä½ æœ‰é‹è¡Œä¸­çš„èˆŠç‰ˆæœ¬ Care Voice:

```bash
# åœæ­¢æ‰€æœ‰èˆŠå®¹å™¨
podman stop care-voice-gpu care-voice-cpu care-voice-unified 2>/dev/null || true
podman rm care-voice-gpu care-voice-cpu care-voice-unified 2>/dev/null || true

# å‚™ä»½é‡è¦æ•¸æ“š (å¦‚æœæœ‰è‡ªå®šç¾©é…ç½®)
podman cp care-voice-gpu:/app/logs ./backup-logs 2>/dev/null || true
podman cp care-voice-gpu:/app/whisper_models ./backup-models 2>/dev/null || true

# éƒ¨ç½² RTX 50 ç³»åˆ—æ–°ç‰ˆæœ¬ (ä½¿ç”¨ CDI)
podman run -d --name care-voice-rtx50 \
    --device nvidia.com/gpu=all \
    -p 8001:8001 \
    -e NVIDIA_VISIBLE_DEVICES=all \
    -e CUDA_VISIBLE_DEVICES=0 \
    -e TORCH_CUDA_ARCH_LIST="6.0;6.1;7.0;7.5;8.0;8.6;8.9;12.0" \
    -e ENABLE_FP16=1 \
    care-voice-rtx50:latest

# é©—è­‰å‡ç´šæˆåŠŸ
curl http://localhost:8001/health
podman exec care-voice-rtx50 python3 /app/gpu_diagnostics_rtx50.py

# ç¢ºèª CDI GPU å­˜å–
podman exec care-voice-rtx50 nvidia-smi
nvidia-ctk cdi list | grep nvidia.com/gpu
```

## ğŸ¯ RTX 50 ç³»åˆ—é–‹ç™¼ç’°å¢ƒè¨­ç½®

### RTX 50 ç³»åˆ—æœ¬åœ°é–‹ç™¼ç’°å¢ƒ

éœ€è¦å®‰è£:
- Python 3.11+
- CUDA Toolkit 12.8
- PyTorch nightly cu128
- NVIDIA é©…å‹• 570+ (æ”¯æ´ RTX 50 ç³»åˆ—)

```bash
# å®‰è£ RTX 50 ç³»åˆ—é–‹ç™¼ç’°å¢ƒ
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128
pip install openai-whisper
pip install supervisord

# æª¢æŸ¥ RTX 50 ç³»åˆ—æ”¯æ´
python3 -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, Arch: {torch.cuda.get_arch_list()}')"

# æœ¬åœ°é‹è¡Œ RTX 50 æœå‹™
cd /app
python3 gpu_whisper_server_rtx50.py
```

### RTX 50 ç³»åˆ— Whisper æ¨¡å‹ç®¡ç†

RTX 50 ç³»åˆ—æ”¯æ´æ‰€æœ‰ Whisper æ¨¡å‹å¤§å°:

```bash
# å°å‹æ¨¡å‹ (å¿«é€Ÿæ¨ç†ï¼Œä½ VRAM)
# è‡ªå‹•ä¸‹è¼‰ï¼Œç„¡éœ€æ‰‹å‹•è¨­ç½®

# ä¸­å‹æ¨¡å‹ (å¹³è¡¡æ€§èƒ½)
model = whisper.load_model("base")

# å¤§å‹æ¨¡å‹ (æœ€é«˜ç²¾åº¦ï¼ŒRTX 50 ç³»åˆ—æ¨è–¦)
model = whisper.load_model("large-v3")

# æª¢æŸ¥æ¨¡å‹ VRAM ä½¿ç”¨
python3 -c "import torch; print(f'VRAM Used: {torch.cuda.memory_allocated()/1024**3:.2f}GB')"
```

## ğŸ“‹ RTX 50 ç³»åˆ—ç³»çµ±éœ€æ±‚

### RTX 50 ç³»åˆ—æ¨è–¦é…ç½® (å¯¦éš›é‹è¡Œç¢ºèª)
- **âœ… GPU**: RTX 5070 Ti (å¯¦éš›éƒ¨ç½²ä¸­ï¼Œ31,250 GFLOPS)
- **âœ… VRAM**: 16GB GDDR7 (RTX 5070 Ti å¯¦éš›é…ç½®)
- **âœ… CUDA**: 12.8 (sm_120 æ¶æ§‹æ”¯æ´ç¢ºèª)
- **âœ… é©…å‹•**: é–‹æºé©…å‹• (ç‰ˆæœ¬ç¢ºèªæ”¯æ´ RTX 50 ç³»åˆ—)
- **âœ… PyTorch**: nightly cu128 ç‰ˆæœ¬ (å¯¦éš›å®‰è£)
- **âœ… å®¹å™¨æŠ€è¡“**: Podman + NVIDIA Container Toolkit 1.17.8 + CDI

### å¤šä¸–ä»£å…¼å®¹é…ç½®
- **RTX 40 ç³»åˆ—**: 8GB+ VRAM (ä¼æ¥­ç´š)
- **RTX 30 ç³»åˆ—**: 6GB+ VRAM (ä¸»æµç´š)
- **RTX 20/GTX 16**: 4GB+ VRAM (åŸºæœ¬ç´š)
- **GTX 10+**: 4GB+ VRAM (å…¼å®¹ç´š)

### å®¹å™¨åŒ–ç’°å¢ƒéœ€æ±‚ (å¯¦éš›éƒ¨ç½²ç¢ºèª)
- **âœ… ä½œæ¥­ç³»çµ±**: Ubuntu 24.04 LTS (å¯¦éš›ä½¿ç”¨)
- **âœ… å®¹å™¨**: Podman 4.0+ (å¯¦éš›ä½¿ç”¨ CDI æ”¯æ´)
- **âœ… RAM**: 4GB+ ç³»çµ±è¨˜æ†¶é«” (å¯¦éš›é‹è¡Œä¸­)
- **âœ… å­˜å„²**: 10GB+ å¯ç”¨ç©ºé–“ (å¯¦éš›å®¹å™¨å¤§å°)
- **âœ… NVIDIA Container Toolkit**: 1.17.8 (å¯¦éš›å®‰è£ç‰ˆæœ¬)

### æ”¯æ´çš„ä½œæ¥­ç³»çµ±
- **âœ… Ubuntu 24.04 LTS** (ä¸»è¦æ”¯æ´ï¼Œå¯¦éš›é‹è¡Œä¸­)
- Red Hat Enterprise Linux 9 (ç†è«–æ”¯æ´)
- SUSE Linux Enterprise 15 (ç†è«–æ”¯æ´)
- Windows 11 + WSL2 + Ubuntu 24.04 (ç†è«–æ”¯æ´)

---

**æœ€å¾Œæ›´æ–°**: 2025-07-24 RTX 50 ç³»åˆ— GPU éƒ¨ç½²å®Œæˆ  
**ç‰ˆæœ¬**: RTX 50 ç³»åˆ— + CDI GPU å­˜å– + æ··åˆç²¾åº¦å„ªåŒ– + Podman åŸç”Ÿæ”¯æ´  
**å¯¦éš›é‹è¡Œ**: RTX 5070 Ti æª¢æ¸¬çµæœ - 31,250 GFLOPS, CUDA 12.8, sm_120