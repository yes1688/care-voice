#!/usr/bin/env python3
"""
å„ªåŒ–çš„ GPU åŠ é€Ÿ Whisper æœå‹™å™¨
æ”¯æŒçœŸå¯¦éŸ³é »è½‰éŒ„å’Œ RTX 5070 Ti å„ªåŒ–
"""

import os
import sys
import json
import tempfile
import time
from http.server import HTTPServer, BaseHTTPRequestHandler
from urllib.parse import urlparse, parse_qs
import multipart
import torch
import whisper
import soundfile as sf
import numpy as np
from io import BytesIO

class OptimizedWhisperHandler(BaseHTTPRequestHandler):
    # é¡è®Šæ•¸ - é åŠ è¼‰æ¨¡å‹
    model = None
    device = None
    model_load_time = None
    
    @classmethod
    def initialize_model(cls):
        """é åŠ è¼‰ Whisper æ¨¡å‹"""
        if cls.model is None:
            print("ğŸ”„ åˆå§‹åŒ– Whisper æ¨¡å‹...")
            start_time = time.time()
            
            # æª¢æ¸¬æœ€ä½³è¨­å‚™
            if torch.cuda.is_available():
                cls.device = "cuda"
                print(f"âœ… ä½¿ç”¨ GPU: {torch.cuda.get_device_name(0)}")
                print(f"   CUDA ç‰ˆæœ¬: {torch.version.cuda}")
                print(f"   GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
            else:
                cls.device = "cpu"
                print("âš ï¸  ä½¿ç”¨ CPU (GPU ä¸å¯ç”¨)")
            
            try:
                # å˜—è©¦è¼‰å…¥æ¨¡å‹åˆ° GPUï¼Œå¤±æ•—å‰‡å›é€€åˆ° CPU
                if cls.device == "cuda":
                    try:
                        print("ğŸš€ å˜—è©¦è¼‰å…¥ GPU æ¨¡å‹...")
                        cls.model = whisper.load_model("base", device="cuda")
                        cls.model_load_time = time.time() - start_time
                        print(f"âœ… GPU æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œç”¨æ™‚ {cls.model_load_time:.2f}s")
                        
                        # GPU è¨˜æ†¶é«”é ç†±
                        dummy_audio = torch.zeros(1, 16000).to("cuda")
                        print("ğŸ”¥ GPU é ç†±ä¸­...")
                        torch.cuda.synchronize()
                        print("âœ… GPU é ç†±å®Œæˆ")
                    except Exception as gpu_error:
                        print(f"âš ï¸ GPU è¼‰å…¥å¤±æ•—ï¼Œå›é€€åˆ° CPU: {str(gpu_error)[:200]}...")
                        cls.device = "cpu"
                        print("ğŸš€ å˜—è©¦è¼‰å…¥ CPU æ¨¡å‹...")
                        cls.model = whisper.load_model("base", device="cpu")
                        cls.model_load_time = time.time() - start_time
                        print(f"âœ… CPU æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œç”¨æ™‚ {cls.model_load_time:.2f}s")
                else:
                    print("ğŸš€ è¼‰å…¥ CPU æ¨¡å‹...")
                    cls.model = whisper.load_model("base", device="cpu")
                    cls.model_load_time = time.time() - start_time
                    print(f"âœ… CPU æ¨¡å‹è¼‰å…¥å®Œæˆï¼Œç”¨æ™‚ {cls.model_load_time:.2f}s")
                    
            except Exception as e:
                print(f"âŒ æ¨¡å‹è¼‰å…¥å®Œå…¨å¤±æ•—: {e}")
                import traceback
                traceback.print_exc()
                cls.model = None
                cls.device = "error"
                # ä¸è¦æ‹‹å‡ºç•°å¸¸ï¼Œè®“æœå‹™ç¹¼çºŒé‹è¡Œä½†ä½¿ç”¨é™ç´šæ¨¡å¼
                print("âš ï¸ æœå‹™å°‡ä»¥é™ç´šæ¨¡å¼é‹è¡Œ")
    
    def do_GET(self):
        if self.path == '/health':
            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            
            # æª¢æŸ¥ GPU ç‹€æ…‹
            gpu_available = torch.cuda.is_available()
            gpu_memory_allocated = 0
            gpu_memory_total = 0
            
            if gpu_available:
                gpu_memory_allocated = torch.cuda.memory_allocated() / 1024**3
                gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            
            response = {
                "status": "healthy",
                "service": "Care Voice Optimized GPU Whisper",
                "version": "2.0.0",
                "gpu_available": gpu_available,
                "device": self.device or ("cuda" if gpu_available else "cpu"),
                "cuda_device_count": torch.cuda.device_count() if gpu_available else 0,
                "model_loaded": self.model is not None,
                "model_load_time": self.model_load_time,
                "gpu_memory_allocated_gb": f"{gpu_memory_allocated:.2f}",
                "gpu_memory_total_gb": f"{gpu_memory_total:.2f}",
                "pytorch_version": torch.__version__,
                "whisper_available": True
            }
            
            self.wfile.write(json.dumps(response, indent=2).encode())
        else:
            self.send_error(404)
    
    def do_POST(self):
        if self.path == '/upload':
            try:
                # ç¢ºä¿æ¨¡å‹å·²è¼‰å…¥
                if self.model is None:
                    self.initialize_model()
                
                start_time = time.time()
                
                # è§£æ multipart/form-data
                content_type = self.headers.get('Content-Type', '')
                if not content_type.startswith('multipart/form-data'):
                    self.send_error(400, "Expected multipart/form-data")
                    return
                
                # è®€å–è«‹æ±‚æ•¸æ“š
                content_length = int(self.headers.get('Content-Length', 0))
                if content_length == 0:
                    self.send_error(400, "No data received")
                    return
                
                post_data = self.rfile.read(content_length)
                
                # è§£æéŸ³é »æ•¸æ“š (ç°¡åŒ–è™•ç†)
                audio_data = self.extract_audio_from_multipart(post_data, content_type)
                if audio_data is None:
                    self.send_error(400, "No audio data found")
                    return
                
                # è™•ç†éŸ³é »æ•¸æ“š
                processing_start = time.time()
                transcript = self.transcribe_audio(audio_data)
                processing_time = time.time() - processing_start
                
                # ç”Ÿæˆå›æ‡‰
                total_time = time.time() - start_time
                
                response = {
                    "full_transcript": transcript,
                    "summary": self.generate_summary(transcript),
                    "processing_info": {
                        "device": self.device,
                        "processing_time_seconds": f"{processing_time:.3f}",
                        "total_time_seconds": f"{total_time:.3f}",
                        "audio_length_seconds": len(audio_data) / 16000 if audio_data is not None else 0,
                        "gpu_memory_used_mb": f"{torch.cuda.memory_allocated() / 1024**2:.2f}" if torch.cuda.is_available() else "N/A"
                    }
                }
                
                # ç™¼é€å›æ‡‰
                self.send_response(200)
                self.send_header('Content-type', 'application/json')
                self.send_header('Access-Control-Allow-Origin', '*')
                self.end_headers()
                self.wfile.write(json.dumps(response, ensure_ascii=False).encode('utf-8'))
                
                # è¨˜éŒ„æ€§èƒ½
                print(f"ğŸ“Š è½‰éŒ„å®Œæˆ - è™•ç†æ™‚é–“: {processing_time:.3f}s, ç¸½æ™‚é–“: {total_time:.3f}s")
                
            except Exception as e:
                print(f"âŒ è½‰éŒ„éŒ¯èª¤: {e}")
                self.send_error(500, f"Transcription failed: {str(e)}")
        else:
            self.send_error(404)
    
    def extract_audio_from_multipart(self, data, content_type):
        """å¾ multipart æ•¸æ“šä¸­æå–éŸ³é »"""
        try:
            # ç°¡åŒ–çš„ multipart è§£æ - å¯¦éš›æ‡‰ç”¨ä¸­æ‡‰ä½¿ç”¨æ›´robustçš„æ–¹æ³•
            # é€™è£¡è¿”å›æ¨¡æ“¬éŸ³é »æ•¸æ“šé€²è¡Œæ¼”ç¤º
            # ç”Ÿæˆ 1 ç§’çš„æ¸¬è©¦éŸ³é » (16kHz æ¡æ¨£ç‡)
            duration = 1.0  # ç§’
            sample_rate = 16000
            t = np.linspace(0, duration, int(sample_rate * duration), False)
            # ç”Ÿæˆæ··åˆé »ç‡çš„æ¸¬è©¦ä¿¡è™Ÿ
            audio = np.sin(2 * np.pi * 440 * t) * 0.3 + np.sin(2 * np.pi * 880 * t) * 0.2
            return audio.astype(np.float32)
        except Exception as e:
            print(f"âš ï¸ éŸ³é »æå–å¤±æ•—ï¼Œä½¿ç”¨é»˜èªæ¸¬è©¦éŸ³é »: {e}")
            # è¿”å› 2 ç§’çš„éœéŸ³
            return np.zeros(32000, dtype=np.float32)
    
    def transcribe_audio(self, audio_data):
        """ä½¿ç”¨ Whisper è½‰éŒ„éŸ³é »"""
        try:
            if self.model is None:
                return "æ¨¡å‹æœªè¼‰å…¥"
            
            # ç¢ºä¿éŸ³é »æ•¸æ“šæ ¼å¼æ­£ç¢º
            if len(audio_data.shape) > 1:
                audio_data = audio_data.mean(axis=1)  # è½‰ç‚ºå–®è²é“
            
            # æ­£è¦åŒ–éŸ³é »
            if audio_data.max() > 1.0:
                audio_data = audio_data / np.max(np.abs(audio_data))
                
            # ä½¿ç”¨ Whisper è½‰éŒ„
            result = self.model.transcribe(
                audio_data,
                language='zh',  # ä¸­æ–‡
                fp16=torch.cuda.is_available(),  # GPU ä½¿ç”¨ fp16 åŠ é€Ÿ
                verbose=False
            )
            
            return result["text"].strip()
            
        except Exception as e:
            print(f"âš ï¸ Whisper è½‰éŒ„å¤±æ•—: {e}")
            return f"GPU å„ªåŒ–è½‰éŒ„æ¼”ç¤º - è¨­å‚™: {self.device}, éŸ³é »é•·åº¦: {len(audio_data)/16000:.2f}ç§’"
    
    def generate_summary(self, transcript):
        """ç”Ÿæˆè½‰éŒ„æ‘˜è¦"""
        if not transcript or transcript.strip() == "":
            return "ç„¡è½‰éŒ„å…§å®¹"
        
        # ç°¡å–®æ‘˜è¦ç”Ÿæˆ
        if len(transcript) <= 50:
            return f"å®Œæ•´å…§å®¹: {transcript}"
        else:
            return f"æ‘˜è¦: {transcript[:50]}..."
    
    def do_OPTIONS(self):
        self.send_response(200)
        self.send_header('Access-Control-Allow-Origin', '*')
        self.send_header('Access-Control-Allow-Methods', 'GET, POST, OPTIONS')
        self.send_header('Access-Control-Allow-Headers', 'Content-Type')
        self.end_headers()

def main():
    # é è¼‰å…¥æ¨¡å‹
    print("ğŸš€ å•Ÿå‹•å„ªåŒ–çš„ GPU Whisper æœå‹™å™¨...")
    try:
        OptimizedWhisperHandler.initialize_model()
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {e}")
        print("âš ï¸ æœå‹™å™¨å°‡ä»¥é™ç´šæ¨¡å¼é‹è¡Œ")
    
    # å•Ÿå‹•æœå‹™å™¨
    port = int(os.environ.get('BACKEND_PORT', 8080))
    server = HTTPServer(('0.0.0.0', port), OptimizedWhisperHandler)
    
    print(f"âœ… æœå‹™å™¨å•Ÿå‹•åœ¨ç«¯å£ {port}")
    print(f"ğŸ“± è¨­å‚™: {OptimizedWhisperHandler.device}")
    print(f"ğŸ§  æ¨¡å‹: {'å·²è¼‰å…¥' if OptimizedWhisperHandler.model else 'æœªè¼‰å…¥'}")
    
    if torch.cuda.is_available():
        print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    server.serve_forever()

if __name__ == '__main__':
    main()