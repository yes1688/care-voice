# ğŸš¨ Care Voice æ•…éšœæ’é™¤æŒ‡å—

## ğŸ¯ å¿«é€Ÿè¨ºæ–·

### ç¬¬ä¸€æ­¥ï¼šåŸºæœ¬æª¢æŸ¥
```bash
# æª¢æŸ¥æœå‹™ç‹€æ…‹
podman ps | grep care-voice

# æª¢æŸ¥æœå‹™å¥åº·
curl http://localhost:8001/health

# æª¢æŸ¥å®¹å™¨æ—¥èªŒ
podman logs care-voice | tail -20
```

## ğŸ”¥ å¸¸è¦‹å•é¡Œè§£æ±º

### 1. å®¹å™¨ç„¡æ³•å•Ÿå‹•

#### ç—‡ç‹€
- `podman run` å‘½ä»¤åŸ·è¡Œå¤±æ•—
- å®¹å™¨ç‹€æ…‹é¡¯ç¤º "Exited"

#### è§£æ±ºæ–¹æ¡ˆ
```bash
# æª¢æŸ¥è©³ç´°éŒ¯èª¤
podman logs care-voice

# æª¢æŸ¥åŸ å£è¡çª
lsof -i :8001

# åœæ­¢è¡çªæœå‹™
podman stop $(podman ps -q --filter "ancestor=care-voice:whisper-rs-gpu")

# æ¸…ç†ä¸¦é‡æ–°å•Ÿå‹•
podman rm care-voice
podman run -d --name care-voice --gpus all -p 8001:8001 care-voice:whisper-rs-gpu
```

### 2. GPU ä¸å¯ç”¨

#### ç—‡ç‹€
- å¥åº·æª¢æŸ¥é¡¯ç¤º `"gpu_available": false`
- è½‰éŒ„é€Ÿåº¦ç•°å¸¸æ…¢

#### è¨ºæ–·å‘½ä»¤
```bash
# æª¢æŸ¥ä¸»æ©Ÿ GPU
nvidia-smi

# æª¢æŸ¥å®¹å™¨å…§ GPU
podman exec -it care-voice nvidia-smi

# æª¢æŸ¥ CUDA ç‰ˆæœ¬
podman exec -it care-voice nvcc --version
```

#### è§£æ±ºæ–¹æ¡ˆ
```bash
# é‡æ–°å®‰è£ NVIDIA Container Toolkit
sudo apt remove nvidia-container-toolkit
sudo apt update
sudo apt install nvidia-container-toolkit

# é‡å•Ÿå®¹å™¨æœå‹™
sudo systemctl restart docker  # å¦‚ä½¿ç”¨ Docker

# æª¢æŸ¥ --gpus åƒæ•¸
podman run -d --name care-voice --gpus all -p 8001:8001 care-voice:whisper-rs-gpu
```

### 3. è¨˜æ†¶é«”ä¸è¶³

#### ç—‡ç‹€
- å®¹å™¨çªç„¶åœæ­¢
- æ—¥èªŒé¡¯ç¤º "Out of memory" éŒ¯èª¤
- GPU è¨˜æ†¶é«”è€—ç›¡

#### è¨ºæ–·å‘½ä»¤
```bash
# æª¢æŸ¥ç³»çµ±è¨˜æ†¶é«”
free -h

# æª¢æŸ¥ GPU è¨˜æ†¶é«”
nvidia-smi --query-gpu=memory.used,memory.total --format=csv

# æª¢æŸ¥å®¹å™¨è³‡æºä½¿ç”¨
podman stats care-voice
```

#### è§£æ±ºæ–¹æ¡ˆ
```bash
# ä½¿ç”¨è¼ƒå°æ¨¡å‹
# å°‡ models/ggml-medium.bin æ›¿æ›ç‚º models/ggml-base.bin

# é™åˆ¶å®¹å™¨è¨˜æ†¶é«”
podman run -d --name care-voice --gpus all \
  --memory=6g --memory-swap=6g \
  -p 8001:8001 care-voice:whisper-rs-gpu

# è¨­ç½®ç’°å¢ƒè®Šæ•¸
podman run -d --name care-voice --gpus all \
  -e CUDA_MEMORY_POOL_DISABLED=1 \
  -p 8001:8001 care-voice:whisper-rs-gpu
```

### 4. æ¨¡å‹è¼‰å…¥å¤±æ•—

#### ç—‡ç‹€
- å¥åº·æª¢æŸ¥é¡¯ç¤º `"model_loaded": false`
- è½‰éŒ„è«‹æ±‚è¿”å›éŒ¯èª¤

#### è¨ºæ–·å‘½ä»¤
```bash
# æª¢æŸ¥æ¨¡å‹æ–‡ä»¶
ls -la models/
file models/ggml-*.bin

# æª¢æŸ¥æ¨¡å‹æ›è¼‰
podman exec -it care-voice ls -la /app/models/
```

#### è§£æ±ºæ–¹æ¡ˆ
```bash
# é‡æ–°ä¸‹è¼‰æ¨¡å‹
rm models/ggml-*.bin
curl -L -o models/ggml-base.bin \
  https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.bin

# æª¢æŸ¥æ–‡ä»¶æ¬Šé™
chmod 644 models/ggml-*.bin

# é‡æ–°æ›è¼‰æ¨¡å‹
podman stop care-voice
podman rm care-voice
podman run -d --name care-voice --gpus all -p 8001:8001 \
  -v $(pwd)/models:/app/models:ro care-voice:whisper-rs-gpu
```

### 5. ç¶²è·¯é€£æ¥å•é¡Œ

#### ç—‡ç‹€
- ç„¡æ³•è¨ªå• http://localhost:8001
- å‰ç«¯ç„¡æ³•é€£æ¥åˆ°å¾Œç«¯

#### è¨ºæ–·å‘½ä»¤
```bash
# æª¢æŸ¥åŸ å£ç¶å®š
netstat -tulpn | grep 8001

# æª¢æŸ¥é˜²ç«ç‰†
sudo ufw status  # Ubuntu
sudo firewall-cmd --list-ports  # Fedora/RHEL

# æª¢æŸ¥å®¹å™¨ç¶²è·¯
podman port care-voice
```

#### è§£æ±ºæ–¹æ¡ˆ
```bash
# é–‹æ”¾é˜²ç«ç‰†åŸ å£
sudo ufw allow 8001  # Ubuntu
sudo firewall-cmd --permanent --add-port=8001/tcp && sudo firewall-cmd --reload  # Fedora/RHEL

# ç¶å®šåˆ°æ‰€æœ‰ä»‹é¢
podman run -d --name care-voice --gpus all -p 0.0.0.0:8001:8001 care-voice:whisper-rs-gpu

# æª¢æŸ¥ SELinux (å¦‚é©ç”¨)
setsebool -P container_connect_any 1
```

### 6. éŸ³é »ä¸Šå‚³å•é¡Œ

#### ç—‡ç‹€
- éŒ„éŸ³åŠŸèƒ½ç„¡æ³•ä½¿ç”¨
- éŸ³é »æ–‡ä»¶ä¸Šå‚³å¤±æ•—

#### ç€è¦½å™¨ç›¸é—œ
```javascript
// æª¢æŸ¥ç€è¦½å™¨æ§åˆ¶å°éŒ¯èª¤
// ç¢ºä¿ä½¿ç”¨ HTTPS æˆ– localhost
// æª¢æŸ¥éº¥å…‹é¢¨æ¬Šé™
```

#### å¾Œç«¯è¨ºæ–·
```bash
# æª¢æŸ¥ä¸Šå‚³ç«¯é»
curl -X POST -F "audio=@test.wav" http://localhost:8001/transcribe

# æª¢æŸ¥æ–‡ä»¶å¤§å°é™åˆ¶
podman exec -it care-voice cat /etc/nginx/nginx.conf | grep client_max_body_size
```

#### è§£æ±ºæ–¹æ¡ˆ
```bash
# å¢åŠ æ–‡ä»¶å¤§å°é™åˆ¶ (å¦‚éœ€è¦)
# ç·¨è¼¯ unified-nginx.conf
client_max_body_size 100M;

# é‡æ–°å»ºæ§‹å®¹å™¨
podman build --no-cache -f Dockerfile.whisper-rs-gpu -t care-voice:whisper-rs-gpu .
```

## ğŸ› ï¸ é€²éšæ•…éšœæ’é™¤

### å®¹å™¨å»ºæ§‹å•é¡Œ

#### CUDA æ˜ åƒä¸‹è¼‰å¤±æ•—
```bash
# ä½¿ç”¨ç‰¹å®šç‰ˆæœ¬
podman build --build-arg CUDA_VERSION=12.1.1 -f Dockerfile.whisper-rs-gpu .

# æ¸…ç†ä¸¦é‡è©¦
podman system prune -a
podman build --no-cache -f Dockerfile.whisper-rs-gpu .
```

#### CMake ç‰ˆæœ¬éŒ¯èª¤
```dockerfile
# åœ¨ Dockerfile ä¸­ç¢ºä¿ CMake ç‰ˆæœ¬
RUN cmake --version  # æ‡‰è©² >= 3.18
```

#### Rust ç·¨è­¯éŒ¯èª¤
```bash
# æª¢æŸ¥ç·¨è­¯ç’°å¢ƒ
podman run -it --rm nvidia/cuda:12.1.1-devel-ubuntu20.04 bash
apt update && apt install build-essential cmake clang libclang-dev
```

### æ•ˆèƒ½å•é¡Œ

#### è½‰éŒ„é€Ÿåº¦æ…¢
```bash
# ç¢ºèª GPU åŠ é€Ÿ
podman exec -it care-voice nvidia-smi

# æª¢æŸ¥æ¨¡å‹å¤§å°
ls -lh models/

# ä½¿ç”¨æ›´å°æ¨¡å‹
# base < medium < large (é€Ÿåº¦ç”±å¿«åˆ°æ…¢)
```

#### é«˜ CPU ä½¿ç”¨ç‡
```bash
# é™åˆ¶ CPU ä½¿ç”¨
podman run -d --name care-voice --gpus all --cpus=2 -p 8001:8001 care-voice:whisper-rs-gpu

# æª¢æŸ¥è³‡æºåˆ†é…
podman stats care-voice
```

## ğŸ“Š ç›£æ§èˆ‡æ—¥èªŒ

### å¯¦æ™‚ç›£æ§
```bash
# GPU ä½¿ç”¨ç›£æ§
watch -n 1 'podman exec care-voice nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv'

# å®¹å™¨è³‡æºç›£æ§
watch -n 1 'podman stats care-voice'

# æœå‹™ç‹€æ…‹ç›£æ§
watch -n 5 'curl -s http://localhost:8001/health | jq'
```

### æ—¥èªŒåˆ†æ
```bash
# æŸ¥çœ‹è©³ç´°æ—¥èªŒ
podman logs -f care-voice

# éæ¿¾éŒ¯èª¤è¨Šæ¯
podman logs care-voice 2>&1 | grep -i error

# æ—¥èªŒè¼ªæ›è¨­ç½®
podman run -d --name care-voice --gpus all \
  --log-driver=journald --log-opt max-size=10m \
  -p 8001:8001 care-voice:whisper-rs-gpu
```

## ğŸ”„ é‡ç½®èˆ‡æ¸…ç†

### å®Œå…¨é‡ç½®
```bash
# åœæ­¢ä¸¦ç§»é™¤å®¹å™¨
podman stop care-voice
podman rm care-voice

# ç§»é™¤æ˜ åƒ
podman rmi care-voice:whisper-rs-gpu

# æ¸…ç†ç³»çµ±
podman system prune -a

# é‡æ–°å»ºæ§‹å’Œéƒ¨ç½²
podman build -f Dockerfile.whisper-rs-gpu -t care-voice:whisper-rs-gpu .
podman run -d --name care-voice --gpus all -p 8001:8001 care-voice:whisper-rs-gpu
```

### ä¿ç•™è³‡æ–™é‡ç½®
```bash
# åªé‡å•Ÿæœå‹™
podman restart care-voice

# é‡æ–°éƒ¨ç½²å®¹å™¨ (ä¿ç•™æ¨¡å‹)
podman stop care-voice && podman rm care-voice
podman run -d --name care-voice --gpus all -p 8001:8001 \
  -v $(pwd)/models:/app/models:ro care-voice:whisper-rs-gpu
```

## ğŸ“ ç²å–å¹«åŠ©

### æ—¥èªŒæ”¶é›†
```bash
# æ”¶é›†ç³»çµ±è³‡è¨Š
{
  echo "=== System Info ==="
  uname -a
  echo "=== GPU Info ==="
  nvidia-smi
  echo "=== Container Info ==="
  podman version
  echo "=== Service Logs ==="
  podman logs care-voice | tail -50
} > care-voice-debug.log
```

### ç¤¾ç¾¤æ”¯æ´
- **GitHub Issues**: [å°ˆæ¡ˆ Issues é é¢]
- **æ–‡æª”**: [å®Œæ•´æŠ€è¡“æ–‡æª”](../technical/)
- **é…ç½®åƒè€ƒ**: [ç³»çµ±é…ç½®](../../claude.md)

---

**æç¤º**: å¤§å¤šæ•¸å•é¡Œå¯ä»¥é€šéé‡æ–°å•Ÿå‹•å®¹å™¨è§£æ±ºã€‚å¦‚æœå•é¡ŒæŒçºŒå­˜åœ¨ï¼Œè«‹æ”¶é›†æ—¥èªŒä¸¦åƒè€ƒ [GPU é…ç½®æŒ‡å—](../technical/gpu-configuration.md)ã€‚