# ğŸš¨ Care Voice æ•…éšœæ’é™¤æŒ‡å—

## ğŸ¯ å¿«é€Ÿè¨ºæ–·

### ç¬¬ä¸€æ­¥ï¼šåŸºæœ¬æª¢æŸ¥
```bash
# æª¢æŸ¥æœå‹™ç‹€æ…‹
podman ps | grep care-voice

# æª¢æŸ¥æœå‹™å¥åº·
curl http://localhost:8001/health

# æª¢æŸ¥å®¹å™¨æ—¥èªŒ
podman logs care-voice-ultimate | tail -20
```

## âš¡ å¿«é€Ÿä¿®å¾©æŒ‡ä»¤

### æœ€å¸¸è¦‹å•é¡Œçš„ä¸€éµä¿®å¾©
```bash
# 1. æœåŠ¡å®Œå…¨é‡å•Ÿ (è§£å†³ 90% é—®é¢˜)
podman restart care-voice-ultimate

# 2. GPU è¨ªå•æ¢å¾©
podman exec care-voice-ultimate python3 /app/gpu_diagnostics.py

# 3. å¥åº·æª¢æŸ¥ç¢ºèª
curl http://localhost:8001/health | jq

# 4. å®¹å™¨é‡æ–°éƒ¨ç½² (å¦‚æœä»¥ä¸Šéƒ½å¤±æ•ˆ)
podman stop care-voice-ultimate && podman rm care-voice-ultimate
podman run -d --name care-voice-ultimate --gpus all -p 8001:8001 \
  -v ./backend/models:/app/models:ro care-voice:whisper-rs-gpu-v2
```

## ğŸ”¥ å¸¸è¦‹å•é¡Œè§£æ±º

### 1. å®¹å™¨ç„¡æ³•å•Ÿå‹•

#### ç—‡ç‹€
- `podman run` å‘½ä»¤åŸ·è¡Œå¤±æ•—
- å®¹å™¨ç‹€æ…‹é¡¯ç¤º "Exited"

#### è§£æ±ºæ–¹æ¡ˆ
```bash
# æª¢æŸ¥è©³ç´°éŒ¯èª¤
podman logs care-voice-ultimate

# æª¢æŸ¥åŸ å£è¡çª
lsof -i :8001

# åœæ­¢è¡çªæœå‹™
podman stop $(podman ps -q --filter "ancestor=care-voice:whisper-rs-gpu-v2")

# æ¸…ç†ä¸¦é‡æ–°å•Ÿå‹•
podman rm care-voice-ultimate
podman run -d --name care-voice-ultimate --gpus all -p 8001:8001 \
  -v ./backend/models:/app/models:ro care-voice:whisper-rs-gpu-v2
```

### 2. GPU ä¸å¯ç”¨

#### ç—‡ç‹€
- å¥åº·æª¢æŸ¥é¡¯ç¤º `"gpu_available": false`
- è½‰éŒ„é€Ÿåº¦ç•°å¸¸æ…¢

#### è¨ºæ–·å‘½ä»¤
```bash
# æª¢æŸ¥ä¸»æ©Ÿ GPU
nvidia-smi

# æª¢æŸ¥å®¹å™¨å…§ GPU
podman exec -it care-voice-ultimate nvidia-smi

# æª¢æŸ¥ CUDA ç‰ˆæœ¬ (æ‡‰ç‚º 12.9.1)
podman exec -it care-voice-ultimate nvcc --version

# ä½¿ç”¨å°ˆç”¨ GPU è¨ºæ–·å·¥å…·
podman exec care-voice-ultimate python3 /app/gpu_diagnostics.py
```

#### è§£æ±ºæ–¹æ¡ˆ
```bash
# é‡æ–°å®‰è£ NVIDIA Container Toolkit
sudo apt remove nvidia-container-toolkit
sudo apt update
sudo apt install nvidia-container-toolkit

# é‡å•Ÿå®¹å™¨æœå‹™
sudo systemctl restart docker  # å¦‚ä½¿ç”¨ Docker

# æª¢æŸ¥ --gpus åƒæ•¸
podman run -d --name care-voice --gpus all -p 8001:8001 care-voice:whisper-rs-gpu
```

### 3. è¨˜æ†¶é«”ä¸è¶³

#### ç—‡ç‹€
- å®¹å™¨çªç„¶åœæ­¢
- æ—¥èªŒé¡¯ç¤º "Out of memory" éŒ¯èª¤
- GPU è¨˜æ†¶é«”è€—ç›¡

#### è¨ºæ–·å‘½ä»¤
```bash
# æª¢æŸ¥ç³»çµ±è¨˜æ†¶é«”
free -h

# æª¢æŸ¥ GPU è¨˜æ†¶é«”
nvidia-smi --query-gpu=memory.used,memory.total --format=csv

# æª¢æŸ¥å®¹å™¨è³‡æºä½¿ç”¨
podman stats care-voice
```

#### è§£æ±ºæ–¹æ¡ˆ
```bash
# ä½¿ç”¨è¼ƒå°æ¨¡å‹
# å°‡ models/ggml-medium.bin æ›¿æ›ç‚º models/ggml-base.bin

# é™åˆ¶å®¹å™¨è¨˜æ†¶é«”
podman run -d --name care-voice --gpus all \
  --memory=6g --memory-swap=6g \
  -p 8001:8001 care-voice:whisper-rs-gpu

# è¨­ç½®ç’°å¢ƒè®Šæ•¸
podman run -d --name care-voice --gpus all \
  -e CUDA_MEMORY_POOL_DISABLED=1 \
  -p 8001:8001 care-voice:whisper-rs-gpu
```

### 4. æ¨¡å‹è¼‰å…¥å¤±æ•—

#### ç—‡ç‹€
- å¥åº·æª¢æŸ¥é¡¯ç¤º `"model_loaded": false`
- è½‰éŒ„è«‹æ±‚è¿”å›éŒ¯èª¤

#### è¨ºæ–·å‘½ä»¤
```bash
# æª¢æŸ¥æ¨¡å‹æ–‡ä»¶
ls -la models/
file models/ggml-*.bin

# æª¢æŸ¥æ¨¡å‹æ›è¼‰
podman exec -it care-voice ls -la /app/models/
```

#### è§£æ±ºæ–¹æ¡ˆ
```bash
# é‡æ–°ä¸‹è¼‰æ¨¡å‹
rm models/ggml-*.bin
curl -L -o models/ggml-base.bin \
  https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.bin

# æª¢æŸ¥æ–‡ä»¶æ¬Šé™
chmod 644 models/ggml-*.bin

# é‡æ–°æ›è¼‰æ¨¡å‹
podman stop care-voice
podman rm care-voice
podman run -d --name care-voice --gpus all -p 8001:8001 \
  -v $(pwd)/models:/app/models:ro care-voice:whisper-rs-gpu
```

### 5. ç¶²è·¯é€£æ¥å•é¡Œ

#### ç—‡ç‹€
- ç„¡æ³•è¨ªå• http://localhost:8001
- å‰ç«¯ç„¡æ³•é€£æ¥åˆ°å¾Œç«¯

#### è¨ºæ–·å‘½ä»¤
```bash
# æª¢æŸ¥åŸ å£ç¶å®š
netstat -tulpn | grep 8001

# æª¢æŸ¥é˜²ç«ç‰†
sudo ufw status  # Ubuntu
sudo firewall-cmd --list-ports  # Fedora/RHEL

# æª¢æŸ¥å®¹å™¨ç¶²è·¯
podman port care-voice
```

#### è§£æ±ºæ–¹æ¡ˆ
```bash
# é–‹æ”¾é˜²ç«ç‰†åŸ å£
sudo ufw allow 8001  # Ubuntu
sudo firewall-cmd --permanent --add-port=8001/tcp && sudo firewall-cmd --reload  # Fedora/RHEL

# ç¶å®šåˆ°æ‰€æœ‰ä»‹é¢
podman run -d --name care-voice --gpus all -p 0.0.0.0:8001:8001 care-voice:whisper-rs-gpu

# æª¢æŸ¥ SELinux (å¦‚é©ç”¨)
setsebool -P container_connect_any 1
```

### 6. éŸ³é »æ ¼å¼è½‰æ›å•é¡Œ â­ (å·²è§£æ±ºæ–¹æ¡ˆ)

#### ç—‡ç‹€
- Chrome/Edge ç€è¦½å™¨éŒ„éŸ³å¾Œå‡ºç¾ `422 Unprocessable Entity` éŒ¯èª¤
- Firefox ç€è¦½å™¨ä¹Ÿå‡ºç¾åŒæ¨£éŒ¯èª¤ (2025å¹´æ›´æ–°)
- éŒ¯èª¤ä¿¡æ¯: "Audio format conversion failed"
- Safari éœ€è¦ HTTPS æ‰èƒ½éŒ„éŸ³

#### æ ¹æœ¬åŸå›  (å·²ç¢ºèª)
**2025å¹´é‡å¤§ç™¼ç¾**: æ‰€æœ‰ç¾ä»£ç€è¦½å™¨éƒ½å·²é·ç§»åˆ° Opus ç·¨ç¢¼å™¨:
- **Chrome/Edge**: `audio/webm;codecs=opus`
- **Firefox**: `audio/ogg;codecs=opus` (å¾ Vorbis é·ç§»)
- **Safari**: `audio/mp4` (AAC ç·¨ç¢¼ï¼Œéœ€è¦ HTTPS)

**æŠ€è¡“é™åˆ¶**: å¾Œç«¯ symphonia 0.5.4 ä¸æ”¯æ´ Opus è§£ç¢¼å™¨

ğŸ“Š **æ·±åº¦åˆ†æ**: [ç€è¦½å™¨éŸ³é »éŒ„è£½å®Œæ•´åˆ†æ](../technical/BROWSER_AUDIO_RECORDING_ANALYSIS.md)

#### å¿«é€Ÿè¨ºæ–·
```bash
# æª¢æŸ¥ç•¶å‰éŒ¯èª¤ç‹€æ³
podman exec care-voice-ultimate grep -E "(Audio conversion failed|ä¸æ”¯æ´)" /var/log/supervisor/whisper-rs.log | tail -5

# æª¢æŸ¥ç€è¦½å™¨ä½¿ç”¨çš„æ ¼å¼
podman logs care-voice-ultimate | grep -E "(webm|ogg|mp4)" | tail -3

# æª¢æŸ¥æœå‹™ç‹€æ…‹
curl -s http://localhost:8001/health | jq
```

#### ğŸš€ æ¨è–¦è§£æ±ºæ–¹æ¡ˆ: Opus å¾Œç«¯è™•ç†
**åŸºæ–¼æ¥­ç•Œæ¨™æº– (Discord/Zoom/Google åŒæ¬¾æŠ€è¡“)**:

##### å®Œæ•´è§£æ±ºæ–¹æ¡ˆæ–‡æª”
- ğŸ¯ **[Opus å¾Œç«¯è™•ç†æ–¹æ¡ˆ](../technical/OPUS_BACKEND_SOLUTION.md)** - æ¥­ç•Œæ¨™æº–æŠ€è¡“æ–¹æ¡ˆ
- ğŸ› ï¸ **[å¯¦æ–½æŒ‡å—](../development/OPUS_IMPLEMENTATION_GUIDE.md)** - è©³ç´°å¯¦æ–½æ­¥é©Ÿ
- ğŸ“Š **[å¤šæ–¹æ¡ˆå°æ¯”](../technical/WEBM_SOLUTION_PLAN.md)** - æŠ€è¡“æ–¹æ¡ˆæ¯”è¼ƒ

##### æ ¸å¿ƒä¿®å¾©æ­¥é©Ÿ
```bash
# 1. å¯¦æ–½ Opus å¾Œç«¯æ”¯æ´ (æ¨è–¦æ–¹æ¡ˆ)
# æ›´æ–° backend/Cargo.toml
[dependencies]
opus = "0.3.0"              # åŸç”Ÿ Opus è§£ç¢¼å™¨
ogg = "0.9.0"               # Firefox OGG å®¹å™¨æ”¯æ´
webm-parser = "0.1.0"       # Chrome WebM å®¹å™¨æ”¯æ´

# 2. é‡å»ºå®¹å™¨ (åŒ…å« Opus æ”¯æ´)
podman build -f Dockerfile.whisper-rs-gpu -t care-voice:opus-support .

# 3. éƒ¨ç½²æ›´æ–°ç‰ˆæœ¬
podman stop care-voice-ultimate && podman rm care-voice-ultimate
podman run -d --name care-voice-ultimate --device /dev/nvidia0 \
  --device /dev/nvidiactl --device /dev/nvidia-uvm \
  -p 8001:8001 \
  -v ./backend/models:/app/models:ro \
  care-voice:opus-support

# 4. é©—è­‰ä¿®å¾©æ•ˆæœ
curl -s http://localhost:8001/health
# æ¸¬è©¦ Chrome/Firefox éŒ„éŸ³åŠŸèƒ½
```

#### é æœŸä¿®å¾©æ•ˆæœ
```
ä¿®å¾©å‰:
Chrome:  âŒ WebM Opus â†’ 422 éŒ¯èª¤
Firefox: âŒ OGG Opus â†’ 422 éŒ¯èª¤
Safari:  â“ éœ€è¦ HTTPS æ¸¬è©¦

ä¿®å¾©å¾Œ:
Chrome:  âœ… WebM Opus â†’ æˆåŠŸè½‰éŒ„
Firefox: âœ… OGG Opus â†’ æˆåŠŸè½‰éŒ„  
Safari:  âœ… MP4 AAC â†’ æˆåŠŸè½‰éŒ„ (HTTPS ç’°å¢ƒ)
```

#### è‡¨æ™‚æ‡‰æ€¥æ–¹æ¡ˆ
å¦‚æœæš«æ™‚ç„¡æ³•å¯¦æ–½ Opus æ”¯æ´ï¼Œå»ºè­°:

1. **å¼•å°ç”¨æˆ¶ä½¿ç”¨ Safari** (éœ€è¦ HTTPS)
2. **æä¾›ç”¨æˆ¶å‹å–„çš„éŒ¯èª¤ä¿¡æ¯**:
   ```
   "âš ï¸ ç€è¦½å™¨éŸ³é »æ ¼å¼æš«æ™‚ä¸æ”¯æ´
   å»ºè­°: 1. ä½¿ç”¨ Safari ç€è¦½å™¨
        2. æˆ–ç­‰å¾…ç³»çµ±æ›´æ–° (Opus æ”¯æ´)"
   ```

### 7. å…¶ä»–éŸ³é »ä¸Šå‚³å•é¡Œ

#### ç—‡ç‹€
- éŒ„éŸ³åŠŸèƒ½ç„¡æ³•ä½¿ç”¨
- éŸ³é »æ–‡ä»¶ä¸Šå‚³å¤±æ•— (éæ ¼å¼å•é¡Œ)

#### ç€è¦½å™¨ç›¸é—œ
```javascript
// æª¢æŸ¥ç€è¦½å™¨æ§åˆ¶å°éŒ¯èª¤
// ç¢ºä¿ä½¿ç”¨ HTTPS æˆ– localhost
// æª¢æŸ¥éº¥å…‹é¢¨æ¬Šé™
```

#### å¾Œç«¯è¨ºæ–·
```bash
# æª¢æŸ¥ä¸Šå‚³ç«¯é»
curl -X POST -F "audio=@test.wav" http://localhost:8001/api/upload

# æª¢æŸ¥æ–‡ä»¶å¤§å°é™åˆ¶
podman exec -it care-voice cat /etc/nginx/nginx.conf | grep client_max_body_size
```

#### è§£æ±ºæ–¹æ¡ˆ
```bash
# å¢åŠ æ–‡ä»¶å¤§å°é™åˆ¶ (å¦‚éœ€è¦)
# ç·¨è¼¯ unified-nginx.conf
client_max_body_size 100M;

# é‡æ–°å»ºæ§‹å®¹å™¨
podman build --no-cache -f Dockerfile.whisper-rs-gpu -t care-voice:whisper-rs-gpu .
```

## ğŸ› ï¸ é€²éšæ•…éšœæ’é™¤

### å®¹å™¨å»ºæ§‹å•é¡Œ

#### CUDA æ˜ åƒä¸‹è¼‰å¤±æ•—
```bash
# ä½¿ç”¨ç‰¹å®šç‰ˆæœ¬
podman build --build-arg CUDA_VERSION=12.1.1 -f Dockerfile.whisper-rs-gpu .

# æ¸…ç†ä¸¦é‡è©¦
podman system prune -a
podman build --no-cache -f Dockerfile.whisper-rs-gpu .
```

#### CMake ç‰ˆæœ¬éŒ¯èª¤
```dockerfile
# åœ¨ Dockerfile ä¸­ç¢ºä¿ CMake ç‰ˆæœ¬
RUN cmake --version  # æ‡‰è©² >= 3.18
```

#### Rust ç·¨è­¯éŒ¯èª¤
```bash
# æª¢æŸ¥ç·¨è­¯ç’°å¢ƒ
podman run -it --rm nvidia/cuda:12.1.1-devel-ubuntu20.04 bash
apt update && apt install build-essential cmake clang libclang-dev
```

### æ•ˆèƒ½å•é¡Œ

#### è½‰éŒ„é€Ÿåº¦æ…¢
```bash
# ç¢ºèª GPU åŠ é€Ÿ
podman exec -it care-voice nvidia-smi

# æª¢æŸ¥æ¨¡å‹å¤§å°
ls -lh models/

# ä½¿ç”¨æ›´å°æ¨¡å‹
# base < medium < large (é€Ÿåº¦ç”±å¿«åˆ°æ…¢)
```

#### é«˜ CPU ä½¿ç”¨ç‡
```bash
# é™åˆ¶ CPU ä½¿ç”¨
podman run -d --name care-voice --gpus all --cpus=2 -p 8001:8001 care-voice:whisper-rs-gpu

# æª¢æŸ¥è³‡æºåˆ†é…
podman stats care-voice
```

## ğŸ“Š ç›£æ§èˆ‡æ—¥èªŒ

### å¯¦æ™‚ç›£æ§
```bash
# GPU ä½¿ç”¨ç›£æ§
watch -n 1 'podman exec care-voice nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv'

# å®¹å™¨è³‡æºç›£æ§
watch -n 1 'podman stats care-voice'

# æœå‹™ç‹€æ…‹ç›£æ§
watch -n 5 'curl -s http://localhost:8001/health | jq'
```

### æ—¥èªŒåˆ†æ
```bash
# æŸ¥çœ‹è©³ç´°æ—¥èªŒ
podman logs -f care-voice

# éæ¿¾éŒ¯èª¤è¨Šæ¯
podman logs care-voice 2>&1 | grep -i error

# æ—¥èªŒè¼ªæ›è¨­ç½®
podman run -d --name care-voice --gpus all \
  --log-driver=journald --log-opt max-size=10m \
  -p 8001:8001 care-voice:whisper-rs-gpu
```

## ğŸ”„ é‡ç½®èˆ‡æ¸…ç†

### å®Œå…¨é‡ç½®
```bash
# åœæ­¢ä¸¦ç§»é™¤å®¹å™¨
podman stop care-voice
podman rm care-voice

# ç§»é™¤æ˜ åƒ
podman rmi care-voice:whisper-rs-gpu

# æ¸…ç†ç³»çµ±
podman system prune -a

# é‡æ–°å»ºæ§‹å’Œéƒ¨ç½²
podman build -f Dockerfile.whisper-rs-gpu -t care-voice:whisper-rs-gpu .
podman run -d --name care-voice --gpus all -p 8001:8001 care-voice:whisper-rs-gpu
```

### ä¿ç•™è³‡æ–™é‡ç½®
```bash
# åªé‡å•Ÿæœå‹™
podman restart care-voice

# é‡æ–°éƒ¨ç½²å®¹å™¨ (ä¿ç•™æ¨¡å‹)
podman stop care-voice && podman rm care-voice
podman run -d --name care-voice --gpus all -p 8001:8001 \
  -v $(pwd)/models:/app/models:ro care-voice:whisper-rs-gpu
```

## ğŸ“ ç²å–å¹«åŠ©

### æ—¥èªŒæ”¶é›†
```bash
# æ”¶é›†ç³»çµ±è³‡è¨Š
{
  echo "=== System Info ==="
  uname -a
  echo "=== GPU Info ==="
  nvidia-smi
  echo "=== Container Info ==="
  podman version
  echo "=== Service Logs ==="
  podman logs care-voice | tail -50
} > care-voice-debug.log
```

### ç¤¾ç¾¤æ”¯æ´èˆ‡ç›¸é—œæ–‡æª”
- **GitHub Issues**: [å°ˆæ¡ˆ Issues é é¢]
- **å®Œæ•´æ–‡æª”ç³»çµ±**:
  - [GPU é…ç½®èˆ‡è¨ºæ–·](../technical/gpu-configuration.md) - è©³ç´°çš„ GPU è¨­ç½®å’Œæ•…éšœæ’é™¤
  - [éƒ¨ç½²æŒ‡å—](../development/deployment-guide.md) - å®¹å™¨éƒ¨ç½²å’Œç›£æ§
  - [ç’°å¢ƒé…ç½®](../development/environment-setup.md) - CUDA 12.9.1 ç’°å¢ƒè¨­ç½®
  - [ç³»çµ±ç‹€æ…‹](../technical/system-status.md) - å°ˆæ¡ˆç•¶å‰ç‹€æ…‹å’Œæˆå°±
- **è§’è‰²å®šç¾©**: [é–‹ç™¼è¦ç¯„](../../claude.md)

### æ•ˆèƒ½ç›£æ§å‘½ä»¤
```bash
# GPU ä½¿ç”¨ç‡å¯¦æ™‚ç›£æ§ (å¾ claude.md)
watch -n 1 'podman exec care-voice-ultimate nvidia-smi'

# å®¹å™¨è³‡æºç›£æ§
podman stats care-voice-ultimate

# æœå‹™å¥åº·ç‹€æ…‹ç›£æ§
watch -n 5 'curl -s http://localhost:8001/health | jq'
```

---

**æç¤º**: 90% çš„å•é¡Œå¯ä»¥é€šé `podman restart care-voice-ultimate` è§£æ±ºã€‚å°æ–¼ GPU ç›¸é—œå•é¡Œï¼Œè«‹ä½¿ç”¨ `python3 /app/gpu_diagnostics.py` è¨ºæ–·å·¥å…·ä¸¦åƒè€ƒ [GPU é…ç½®æŒ‡å—](../technical/gpu-configuration.md)ã€‚