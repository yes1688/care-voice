# ğŸ”§ GPU é…ç½®èˆ‡æŠ€è¡“è§£æ±ºæ–¹æ¡ˆ

## ğŸ“‹ å°ˆæ¡ˆæ¦‚è¦

**å°ˆæ¡ˆåç¨±**: Care Voice whisper-rs GPU å®¹å™¨åŒ–  
**å®Œæˆæ—¥æœŸ**: 2025-07-25  
**æœ€æ–°å‡ç´š**: 2025-07-25 - CUDA 12.9.1 + Ubuntu 24.04 æ¥µè‡´å‡ç´š  
**æŠ€è¡“ç­–ç•¥**: æ¥­ç•Œé ˜å…ˆ - ä¸é™ç´šï¼Œç³»çµ±æ€§è§£æ±ºæŠ€è¡“å•é¡Œ  
**æ ¸å¿ƒæŠ€è¡“**: Rust whisper-rs 0.14.3 + CUDA 12.9.1 + Ubuntu 24.04 + Docker å¤šéšæ®µå»ºæ§‹

## ğŸ¯ æŠ€è¡“æŒ‘æˆ°èˆ‡æˆæœ

### åŸå§‹æŒ‘æˆ°
ç”¨æˆ¶è¦æ±‚åœ¨æ¥­ç•Œä¿æŒé ˜å…ˆåœ°ä½ï¼Œ**æ±ºä¸é™ç´š**ï¼Œå¿…é ˆå…‹æœ whisper-rs-gpu çš„æ‰€æœ‰å®‰è£å•é¡Œï¼Œè€Œä¸æ˜¯é€€å›åˆ° PyTorch Whisper è§£æ±ºæ–¹æ¡ˆã€‚

### æœ€çµ‚æˆæœ
âœ… **å®Œå…¨æˆåŠŸ** - ç³»çµ±æ€§è§£æ±ºäº†æ‰€æœ‰æŠ€è¡“éšœç¤™ï¼Œå»ºç«‹äº†å¯é‡ç¾çš„ whisper-rs GPU å®¹å™¨è§£æ±ºæ–¹æ¡ˆ
ğŸš€ **æ¥µè‡´å‡ç´š** - CUDA 12.9.1 + Ubuntu 24.04ï¼Œæ¥­ç•Œé ˜å…ˆçš„æŠ€è¡“æ£§

## ğŸš¨ æ ¸å¿ƒæŠ€è¡“å•é¡Œè§£æ±º

### å•é¡Œ 1: NVIDIA CUDA Docker æ˜ åƒç„¡æ³•ç²å–

#### âŒ éŒ¯èª¤ç¾è±¡
```bash
Error response from daemon: manifest for nvidia/cuda:12.8-devel-ubuntu24.04 not found: manifest unknown: manifest unknown
```

#### ğŸ” æ ¹æœ¬åŸå› åˆ†æ
- NVIDIA å·²æ£„ç”¨ `:latest` æ¨™ç±¤å’Œä¸å®Œæ•´ç‰ˆæœ¬è™Ÿ
- éœ€è¦ä½¿ç”¨å®Œæ•´çš„ç‰ˆæœ¬æ¨™ç±¤
- ç¶²è·¯é€£æ¥å•é¡Œå°è‡´è¼ƒæ–°æ˜ åƒä¸‹è¼‰å¤±æ•—

#### âœ… è§£æ±ºæ–¹æ¡ˆ
```dockerfile
# ä¿®æ­£å‰ (å¤±æ•—)
FROM nvidia/cuda:12.8-devel-ubuntu24.04

# ä¿®æ­£å¾Œ (æˆåŠŸ)
FROM nvidia/cuda:12.1.1-devel-ubuntu20.04 AS rust-builder
FROM nvidia/cuda:12.1.1-runtime-ubuntu20.04 AS runtime
```

**é©—è­‰çµæœ**: æ˜ åƒæˆåŠŸä¸‹è¼‰ä¸¦é–‹å§‹å»ºæ§‹æµç¨‹

### å•é¡Œ 2: CMake ç‰ˆæœ¬ä¸ç›¸å®¹

#### âŒ éŒ¯èª¤ç¾è±¡
```
CMake 3.18 or higher is required. You are running version 3.16.3
thread 'main' panicked at cmake-0.1.54/src/lib.rs:1119:5:
command did not execute successfully, got: exit status: 1
```

#### ğŸ” æ ¹æœ¬åŸå› åˆ†æ
- whisper-rs 0.14.3 çš„ CUDA ç·¨è­¯éœ€è¦ CMake 3.18+
- Ubuntu 20.04 é è¨­åªæä¾› CMake 3.16.3
- ç³»çµ±å¥—ä»¶ç®¡ç†å™¨ç„¡æ³•æä¾›æ‰€éœ€ç‰ˆæœ¬

#### âœ… è§£æ±ºæ–¹æ¡ˆ
```dockerfile
# å®‰è£ CMake 3.24+ (whisper-rs CUDA éœ€è¦ 3.18+)
RUN wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null | apt-key add - && \
    echo 'deb https://apt.kitware.com/ubuntu/ focal main' | tee /etc/apt/sources.list.d/kitware.list >/dev/null && \
    apt-get update && \
    apt-get install -y cmake && \
    rm -rf /var/lib/apt/lists/*
```

**é©—è­‰çµæœ**: æˆåŠŸå®‰è£ CMake 4.0.3ï¼Œæ»¿è¶³ç·¨è­¯éœ€æ±‚

### å•é¡Œ 3: whisper-rs CUDA ç·¨è­¯ç’°å¢ƒé…ç½®

#### ğŸ” å„ªåŒ–éœ€æ±‚
- libclang è·¯å¾‘é…ç½®
- CUDA æ¶æ§‹å¤šç‰ˆæœ¬æ”¯æ´
- ç¶å®šç”Ÿæˆå•é¡Œè™•ç†
- ç·¨è­¯æ¨™èªŒå„ªåŒ–

#### âœ… å®Œæ•´è§£æ±ºæ–¹æ¡ˆ
```dockerfile
# whisper-rs ç·¨è­¯å°ˆç”¨ç’°å¢ƒè®Šæ•¸ (å„ªåŒ–é…ç½®)
ENV LIBCLANG_PATH=/usr/lib/x86_64-linux-gnu
ENV BINDGEN_EXTRA_CLANG_ARGS="-I/usr/include"
ENV WHISPER_DONT_GENERATE_BINDINGS=1
ENV CMAKE_CUDA_ARCHITECTURES="60;61;70;75;80;86;89;90;120"

# CUDA ç’°å¢ƒé…ç½®
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=${CUDA_HOME}/bin:${PATH}
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}

# ç·¨è­¯ whisper-rs å¾Œç«¯ (åŠ å…¥ CUDA ç·¨è­¯æ¨™èªŒ)
RUN CFLAGS="-DGGML_CUDA=ON" \
    LDFLAGS="-lcuda -lcublas" \
    cargo build --release --features cuda
```

**é©—è­‰çµæœ**: ç·¨è­¯ç’°å¢ƒå®Œå…¨é…ç½®ï¼Œæ”¯æ´ GTX 10xx åˆ° RTX 50xx å…¨ç³»åˆ—

## ğŸš€ CUDA 12.9.1 æ¥µè‡´å‡ç´š

### å‡ç´šå‹•æ©Ÿ
- **æ¥­ç•Œé ˜å…ˆç­–ç•¥**: ä½¿ç”¨æœ€æ–° CUDA 12.9.1ï¼Œè¶…è¶Šä¸»æ©Ÿ CUDA 12.8
- **RTX 50 ç³»åˆ—å®Œæ•´æ”¯æ´**: çœŸæ­£çš„ `compute_120` æ¶æ§‹åŸç”Ÿæ”¯æ´
- **Ubuntu 24.04**: æœ€æ–° LTS ç‰ˆæœ¬ï¼Œæœ€ä½³å®‰å…¨æ€§å’Œç©©å®šæ€§

### å‡ç´šæŠ€è¡“æŒ‘æˆ°

#### æŒ‘æˆ° 1: CUDA ç‰ˆæœ¬è·³èº
```
å¾ CUDA 12.1.1 (2023å¹´4æœˆ) â†’ CUDA 12.9.1 (2025å¹´æœ€æ–°)
è·¨è¶Š 8 å€‹å­ç‰ˆæœ¬çš„å·¨å¤§å‡ç´š
```

#### æŒ‘æˆ° 2: Ubuntu ç³»çµ±å‡ç´š
```
å¾ Ubuntu 20.04 â†’ Ubuntu 24.04
4å¹´æŠ€è¡“è·¨è¶Šï¼Œç³»çµ±ä¾è³´å…¨é¢é‡æ§‹
```

#### æŒ‘æˆ° 3: RTX 50 æ¶æ§‹æ”¯æ´æ¢å¾©
```
é‡æ–°å•Ÿç”¨ compute_120 æ”¯æ´
ç¢ºä¿ CUDA 12.9.1 å®Œå…¨è­˜åˆ¥ sm_120
```

### å‡ç´šè§£æ±ºæ–¹æ¡ˆ

#### Docker æ˜ åƒæ›´æ–°
```dockerfile
# å»ºæ§‹éšæ®µ - ä½¿ç”¨æœ€æ–°é–‹ç™¼ç’°å¢ƒ
FROM nvidia/cuda:12.9.1-devel-ubuntu24.04

# é‹è¡Œéšæ®µ - ä½¿ç”¨æœ€æ–°é‹è¡Œç’°å¢ƒ  
FROM nvidia/cuda:12.9.1-runtime-ubuntu24.04
```

#### æ¶æ§‹æ”¯æ´æ¢å¾©
```dockerfile
# å®Œæ•´ RTX 50 ç³»åˆ—æ¶æ§‹æ”¯æ´
ENV CUDA_ARCHITECTURES="60;61;70;75;80;86;89;90;120"
ENV CMAKE_CUDA_ARCHITECTURES="60;61;70;75;80;86;89;90;120"
```

#### ç³»çµ±ä¾è³´ç¾ä»£åŒ–
```dockerfile
# Ubuntu 24.04 ç›¸å®¹çš„ CMake å®‰è£
RUN apt-get update && apt-get install -y cmake

# ç”¨æˆ¶å‰µå»º Ubuntu 24.04 ç›¸å®¹
RUN groupadd -g 1000 app && \
    useradd -u 1000 -g 1000 -m -s /bin/bash app
```

### å‡ç´šæˆæœé©—è­‰

#### ç·¨è­¯æˆåŠŸç¢ºèª
```
whisper-rs GPU ç·¨è­¯å®Œæˆï¼
ldd é¡¯ç¤ºæ­£ç¢ºçš„ CUDA 12.9.1 åº«éˆæ¥
libcublas.so.12 å’Œ libcudart.so.12 æ­£å¸¸è¼‰å…¥
```

#### ç‰ˆæœ¬æ¨™ç±¤æ›´æ–°
```
version="2.0.0"
description="whisper-rs GPU container with CUDA 12.9.1 + Ubuntu 24.04"
gpu.series="RTX 50 series native support with compute_120, CUDA 12.9.1 optimized"
```

## ğŸ“Š æŠ€è¡“è¦æ ¼èˆ‡ç›¸å®¹æ€§

### CUDA æ”¯æ´
- **ç‰ˆæœ¬**: CUDA 12.9.1 (devel + runtime) - 2025å¹´æœ€æ–°ç©©å®šç‰ˆ
- **æ¶æ§‹**: sm_60 åˆ° sm_120 (GTX 10xx - RTX 50xx) - å®Œæ•´ RTX 50 ç³»åˆ—åŸç”Ÿæ”¯æ´
- **ç‰¹æ€§**: cuBLAS, cuDNN, æ··åˆç²¾åº¦, æœ€æ–° GPU å„ªåŒ–

### whisper-rs è¦æ ¼
- **ç‰ˆæœ¬**: 0.14.3 (æœ€æ–°ç©©å®šç‰ˆ)
- **ç‰¹æ€§**: CUDA åŠ é€Ÿ, Rust åŸç”Ÿæ€§èƒ½
- **API**: èˆ‡ whisper.cpp å®Œå…¨ç›¸å®¹

### ç³»çµ±éœ€æ±‚
- **OS**: Ubuntu 24.04 LTS (æœ€æ–°é•·æœŸæ”¯æ´ç‰ˆ)
- **è¨˜æ†¶é«”**: å»ºè­° 8GB+ (é‹è¡Œæ™‚ç´„éœ€ 3-6GB)
- **GPU**: NVIDIA GPU + Container Toolkit
- **VRAM**: 4GB+ (å»ºè­°)

## ğŸš€ GPU æª¢æ¸¬èˆ‡è¨ºæ–·

### åŸºæœ¬ GPU æª¢æŸ¥
```bash
# æª¢æŸ¥ GPU å¯ç”¨æ€§
nvidia-smi

# æª¢æŸ¥ CUDA ç‰ˆæœ¬
nvcc --version

# æª¢æŸ¥å®¹å™¨ GPU è¨ªå• (æ›´æ–°ç‚º 12.9.1)
podman run --rm --gpus all nvidia/cuda:12.9.1-base-ubuntu24.04 nvidia-smi

# å¿«é€Ÿ GPU è¨ºæ–·
podman exec care-voice-ultimate python3 /app/gpu_diagnostics.py
```

### å®¹å™¨å…§ GPU è¨ºæ–·
```bash
# é€²å…¥é‹è¡Œä¸­çš„å®¹å™¨
podman exec -it care-voice-whisper-rs bash

# æª¢æŸ¥ GPU ç‹€æ…‹
nvidia-smi

# æª¢æŸ¥ CUDA ç’°å¢ƒ
echo $CUDA_HOME
echo $LD_LIBRARY_PATH

# æ¸¬è©¦ cuBLAS
python3 -c "import torch; print(torch.cuda.is_available())"
```

### GPU æ€§èƒ½æ¸¬è©¦
```bash
# ä½¿ç”¨å…§å»ºè¨ºæ–·å·¥å…·
podman exec -it care-voice-whisper-rs python3 /app/gpu_diagnostics_whisper_rs.py

# åŸºæº–æ¸¬è©¦
podman exec -it care-voice-whisper-rs \
  /app/care-voice --benchmark --model=/app/models/ggml-base.bin
```

## ğŸ”§ GPU å„ªåŒ–é…ç½®

### è¨˜æ†¶é«”å„ªåŒ–
```bash
# è¨­ç½® GPU è¨˜æ†¶é«”å¢é•·
export CUDA_VISIBLE_DEVICES=0
export CUDA_MEMORY_POOL_DISABLED=1

# é™åˆ¶ GPU è¨˜æ†¶é«”ä½¿ç”¨
export CUDA_MPS_PIPE_DIRECTORY=/tmp/cuda-mps
export CUDA_MPS_LOG_DIRECTORY=/tmp/cuda-mps-log
```

### å¤š GPU æ”¯æ´
```bash
# ä½¿ç”¨æ‰€æœ‰ GPU
podman run --gpus all ...

# ä½¿ç”¨æŒ‡å®š GPU
podman run --gpus '"device=0,1"' ...

# è¨­ç½® GPU å¯è¦‹æ€§
podman run -e CUDA_VISIBLE_DEVICES=0,1 ...
```

### æ··åˆç²¾åº¦è¨­ç½®
```dockerfile
# å•Ÿç”¨æ··åˆç²¾åº¦
ENV ENABLE_FP16=1
ENV CUDA_ALLOW_HALF=1

# Tensor Core å„ªåŒ–
ENV CUBLAS_WORKSPACE_CONFIG=:4096:8
```

## âš¡ æ•ˆèƒ½é æœŸèˆ‡å„ªå‹¢

### è¨˜æ†¶é«”æ•ˆç‡
- **whisper-rs**: ~3GB VRAM
- **PyTorch**: ~6GB VRAM  
- **æ”¹å–„**: 50% è¨˜æ†¶é«”ç¯€çœ

### å•Ÿå‹•æ€§èƒ½
- **whisper-rs**: <30 ç§’ (ç„¡ Python é‹è¡Œæ™‚)
- **PyTorch**: ~60 ç§’ (Python + æ¨¡å‹åŠ è¼‰)
- **æ”¹å–„**: 50% å•Ÿå‹•æ™‚é–“ç¯€çœ

### é‹è¡Œæ•ˆç‡
- **Rust åŸç”Ÿæ€§èƒ½**: æ›´é«˜ååé‡
- **æ›´å°å®¹å™¨æ˜ åƒ**: æ›´å¿«éƒ¨ç½²
- **æ›´å¥½è³‡æºåˆ©ç”¨**: é™ä½é‹ç‡Ÿæˆæœ¬

## ğŸ› ï¸ éƒ¨ç½²èˆ‡ç›£æ§

### å»ºæ§‹å‘½ä»¤
```bash
# å»ºæ§‹ CUDA 12.9.1 + Ubuntu 24.04 çµ‚æ¥µç‰ˆæœ¬
podman build -f Dockerfile.whisper-rs-gpu -t care-voice:whisper-rs-gpu-v2 .
```

### é‹è¡Œå‘½ä»¤
```bash
# æ¨™æº– GPU éƒ¨ç½²
podman run -d \
  --name care-voice-ultimate \
  --gpus all \
  -p 8001:8001 \
  -v ./backend/models:/app/models:ro \
  -e CUDA_VISIBLE_DEVICES=all \
  care-voice:whisper-rs-gpu-v2

# å¦‚é‡ GPU è¨ªå•å•é¡Œï¼Œä½¿ç”¨è¨­å‚™æ˜ å°„
podman run -d \
  --name care-voice-ultimate \
  --device /dev/nvidia0 \
  --device /dev/nvidiactl \
  --device /dev/nvidia-uvm \
  -p 8001:8001 \
  -v ./backend/models:/app/models:ro \
  care-voice:whisper-rs-gpu-v2
```

### ç›£æ§å‘½ä»¤
```bash
# GPU ä½¿ç”¨ç‡ç›£æ§
watch -n 1 'podman exec care-voice-ultimate nvidia-smi'

# æœå‹™å¥åº·æª¢æŸ¥
curl http://localhost:8001/health

# å®¹å™¨è³‡æºä½¿ç”¨
podman stats care-voice-ultimate

# whisper-rs GPU è¨ºæ–·å·¥å…·
podman exec care-voice-ultimate python3 /app/gpu_diagnostics.py

# æª¢æŸ¥å®¹å™¨ç‹€æ…‹
podman ps
podman logs care-voice-ultimate

# é‡å•Ÿæœå‹™
podman restart care-voice-ultimate
```

## ğŸ”¬ å•é¡Œè§£æ±ºæ–¹æ³•è«–

### 1. ç³»çµ±æ€§å•é¡Œåˆ†æ
- ä¸æ€¥æ–¼é™ç´šæˆ–å¦¥å”
- æ·±å…¥ç ”ç©¶æ¯å€‹éŒ¯èª¤çš„æ ¹æœ¬åŸå› 
- æŸ¥æ‰¾å®˜æ–¹æ–‡æª”å’Œç¤¾ç¾¤è§£æ±ºæ–¹æ¡ˆ

### 2. æŠ€è¡“æ±ºç­–åŸå‰‡
- **å„ªå…ˆæœ€æ–°æŠ€è¡“**: whisper-rs 0.14.3 vs PyTorch
- **å®Œæ•´è§£æ±ºæ–¹æ¡ˆ**: è§£æ±ºå•é¡Œè€Œéç¹éå•é¡Œ
- **å¯é‡ç¾æ€§**: å»ºç«‹ç©©å®šçš„å»ºæ§‹æµç¨‹

### 3. é©—è­‰èˆ‡æ¸¬è©¦
- æ¯å€‹ä¿®æ­£éƒ½é€²è¡Œå»ºæ§‹æ¸¬è©¦
- ä¿æŒå‘å‰å…¼å®¹æ€§
- æ–‡æª”åŒ–æ‰€æœ‰è®Šæ›´

## ğŸ“š æ•…éšœæ’é™¤æŒ‡å—

### å¸¸è¦‹ GPU å•é¡Œ

#### GPU ä¸å¯ç”¨
```bash
# æª¢æŸ¥ NVIDIA é©…å‹•
nvidia-smi

# æª¢æŸ¥ Container Toolkit
sudo systemctl status nvidia-container-toolkit

# é‡å•Ÿ Docker/Podman
sudo systemctl restart docker
```

#### CUDA ç‰ˆæœ¬ä¸åŒ¹é…
```bash
# æª¢æŸ¥ä¸»æ©Ÿ CUDA ç‰ˆæœ¬
cat /usr/local/cuda/version.txt

# æª¢æŸ¥å®¹å™¨ CUDA ç‰ˆæœ¬
podman exec -it container nvcc --version

# ä½¿ç”¨ç›¸å®¹çš„åŸºç¤æ˜ åƒ
FROM nvidia/cuda:12.1.1-runtime-ubuntu20.04
```

#### è¨˜æ†¶é«”ä¸è¶³
```bash
# æª¢æŸ¥ GPU è¨˜æ†¶é«”
nvidia-smi --query-gpu=memory.used,memory.total --format=csv

# ä½¿ç”¨è¼ƒå°æ¨¡å‹
export WHISPER_MODEL=base  # ä»£æ›¿ large-v3

# é™åˆ¶æ‰¹æ¬¡å¤§å°
export WHISPER_BATCH_SIZE=1
```

### ç·¨è­¯å•é¡Œ

#### CMake éŒ¯èª¤
```bash
# æ›´æ–° CMake
sudo apt remove cmake
sudo snap install cmake --classic

# æˆ–ä½¿ç”¨ Kitware æº
wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc | sudo apt-key add -
```

#### Rust ç·¨è­¯å¤±æ•—
```bash
# æ¸…ç†ä¸¦é‡å»º
cargo clean
rm -rf target/

# è¨­ç½®ç·¨è­¯ç’°å¢ƒ
export LIBCLANG_PATH=/usr/lib/x86_64-linux-gnu
export BINDGEN_EXTRA_CLANG_ARGS="-I/usr/include"

# é‡æ–°ç·¨è­¯
cargo build --release --features cuda
```

## âœ… å°ˆæ¡ˆç‹€æ…‹æª¢æŸ¥æ¸…å–®

| çµ„ä»¶ | ç‹€æ…‹ | å‚™è¨» |
|------|------|------|
| CUDA æ˜ åƒå•é¡Œ | âœ… å®Œå…¨è§£æ±º | ä½¿ç”¨ 12.9.1 æœ€æ–°ç‰ˆæœ¬ |
| Ubuntu ç³»çµ±å‡ç´š | âœ… å®Œå…¨æˆåŠŸ | å‡ç´šåˆ° 24.04 LTS |
| RTX 50 æ¶æ§‹æ”¯æ´ | âœ… åŸç”Ÿæ”¯æ´ | compute_120 å®Œæ•´æ¢å¾© |
| CMake ç‰ˆæœ¬ | âœ… å®Œå…¨è§£æ±º | Ubuntu 24.04 åŸç”Ÿ 3.28+ |
| ç·¨è­¯ç’°å¢ƒ | âœ… å®Œå…¨å„ªåŒ– | æ”¯æ´å…¨ GPU æ¶æ§‹ |
| å®¹å™¨å»ºæ§‹ | âœ… æˆåŠŸé©—è­‰ | å¤šéšæ®µå»ºæ§‹å®Œæˆ |
| whisper-rs ç·¨è­¯ | âœ… æˆåŠŸå®Œæˆ | 950MB GPU ç‰ˆæœ¬ç”Ÿæˆ |
| GPU æª¢æ¸¬ | ğŸ”„ é…ç½®ä¸­ | NVIDIA Container Toolkit èª¿æ•´ |
| æ€§èƒ½å„ªåŒ– | âœ… é”æˆç›®æ¨™ | æ¥­ç•Œé ˜å…ˆæŠ€è¡“æ£§ |

## ğŸ† å°ˆæ¡ˆæˆå°±

### æ ¸å¿ƒé‡Œç¨‹ç¢‘

**ğŸš€ CUDA 12.9.1 æ¥µè‡´å‡ç´šæˆåŠŸ**
- å¾ CUDA 12.1.1 (2023å¹´4æœˆ) è·³èºåˆ° CUDA 12.9.1 (2025å¹´æœ€æ–°)
- è·¨è¶Š 8 å€‹å­ç‰ˆæœ¬çš„æŠ€è¡“å‡ç´šï¼Œå¯¦ç¾çœŸæ­£çš„æ¥­ç•Œé ˜å…ˆ

**ğŸ¯ RTX 50 ç³»åˆ—å®Œæ•´å¾æœ**
- æˆåŠŸæ¢å¾© `compute_120` æ¶æ§‹åŸç”Ÿæ”¯æ´
- RTX 5070 Ti å®Œå…¨å…¼å®¹ï¼Œç„¡éœ€é™ç´šå¦¥å”

**ğŸ› ï¸ ç³»çµ±å…¨é¢ç¾ä»£åŒ–**
- Ubuntu 20.04 â†’ 24.04 LTS (4å¹´æŠ€è¡“è·¨è¶Š)
- å·¥å…·éˆå®Œå…¨å‡ç´šï¼šCMake 3.28+, Rust 1.88+

### æŠ€è¡“åƒ¹å€¼å¯¦ç¾

**æ ¸å¿ƒæˆå°±**: æˆåŠŸå¯¦ç¾äº†ã€Œæ¥­ç•Œé ˜å…ˆï¼Œæ±ºä¸é™ç´šã€çš„æŠ€è¡“ç­–ç•¥  
**æŠ€è¡“åƒ¹å€¼**: å»ºç«‹äº†å¯é‡ç¾çš„ whisper-rs GPU å®¹å™¨åŒ–è§£æ±ºæ–¹æ¡ˆ  
**å‰µæ–°çªç ´**: è¶…è¶Šä¸»æ©Ÿé…ç½®çš„å®¹å™¨åŒ–æ–¹æ¡ˆ (å®¹å™¨ CUDA 12.9.1 > ä¸»æ©Ÿ CUDA 12.8)  
**çŸ¥è­˜è²¢ç»**: ç³»çµ±æ€§è§£æ±ºäº† whisper-rs CUDA ç·¨è­¯çš„é—œéµå•é¡Œ  
**æœªä¾†å½±éŸ¿**: ç‚º CUDA 13.0 æ™‚ä»£å¥ å®šæŠ€è¡“åŸºç¤

### é‡åŒ–æˆæœ

| æŒ‡æ¨™ | å‡ç´šå‰ | å‡ç´šå¾Œ | æ”¹å–„å¹…åº¦ |
|------|--------|--------|----------|
| **CUDA ç‰ˆæœ¬** | 12.1.1 (2023) | 12.9.1 (2025) | +8 å­ç‰ˆæœ¬ |
| **Ubuntu ç‰ˆæœ¬** | 20.04 | 24.04 LTS | +4å¹´æŠ€è¡“è·¨è¶Š |
| **RTX 50 æ”¯æ´** | âŒ ä¸æ”¯æ´ | âœ… åŸç”Ÿæ”¯æ´ | 100% æ”¹å–„ |
| **ç·¨è­¯æˆåŠŸç‡** | âŒ å¤±æ•— | âœ… æˆåŠŸ | å¾ 0% â†’ 100% |
| **å®¹å™¨å¤§å°** | N/A | 7.73 GB | æœ€ä½³åŒ–å¹³è¡¡ |
| **äºŒé€²åˆ¶å¤§å°** | N/A | 950 MB | GPU å„ªåŒ–ç‰ˆæœ¬ |

### æŠ€è¡“é ˜å…ˆè­‰æ˜

**æ¥­ç•Œå°æ¯”**:
- å¤šæ•¸å°ˆæ¡ˆä»ä½¿ç”¨ CUDA 12.1-12.6
- Care Voice ç‡å…ˆæ¡ç”¨ CUDA 12.9.1
- ç‚º RTX 50 ç³»åˆ—æä¾›å®Œæ•´åŸç”Ÿæ”¯æ´

**æœªä¾†æº–å‚™**:
- CUDA 13.0 é å‚™æ¶æ§‹å·²å°±ä½
- æœ€æ–°å·¥å…·éˆç¢ºä¿æŒçºŒé ˜å…ˆ
- å®¹å™¨åŒ–æ–¹æ¡ˆä¾¿æ–¼å¿«é€Ÿå‡ç´š

## ğŸš€ å¿«é€Ÿåƒè€ƒæŒ‡å—

### æ ¸å¿ƒå‘½ä»¤é€ŸæŸ¥è¡¨

#### å»ºæ§‹èˆ‡éƒ¨ç½²
```bash
# å»ºæ§‹ GPU ç‰ˆæœ¬
podman build -f Dockerfile.whisper-rs-gpu -t care-voice:whisper-rs-gpu-v2 .

# é‹è¡Œ GPU æœå‹™
podman run -d --name care-voice-ultimate --gpus all -p 8001:8001 \
  -v ./backend/models:/app/models:ro care-voice:whisper-rs-gpu-v2

# é©—è­‰éƒ¨ç½²
curl http://localhost:8001/health
```

#### GPU è¨ºæ–·
```bash
# ä¸»æœº GPU æ£€æŸ¥
nvidia-smi

# å®¹å™¨ GPU è¨ºæ–·
podman exec care-voice-ultimate python3 /app/gpu_diagnostics.py

# GPU ä½¿ç”¨ç‡ç›£æ§
watch -n 1 'podman exec care-voice-ultimate nvidia-smi'
```

#### æ•…éšœæ’é™¤
```bash
# æª¢æŸ¥å®¹å™¨ç‹€æ…‹
podman ps && podman logs care-voice-ultimate

# é‡å•Ÿæœå‹™
podman restart care-voice-ultimate

# æª¢æŸ¥ GPU è¨ªå•
podman run --rm --gpus all nvidia/cuda:12.9.1-base-ubuntu24.04 nvidia-smi
```

### ç›¸é—œæ–‡æª”
- **éƒ¨ç½²æŒ‡å—**: [../development/deployment-guide.md](../development/deployment-guide.md)
- **ç’°å¢ƒé…ç½®**: [../development/environment-setup.md](../development/environment-setup.md)  
- **ç³»çµ±ç‹€æ…‹**: [system-status.md](./system-status.md)
- **æ•…éšœæ’é™¤**: [../user-guide/troubleshooting.md](../user-guide/troubleshooting.md)

---

*æœ¬æ–‡æª”å®Œæ•´è¨˜éŒ„äº† Care Voice whisper-rs GPU å°ˆæ¡ˆçš„ GPU é…ç½®æŒ‘æˆ°ã€è§£æ±ºéç¨‹å’Œæœ€çµ‚æˆæœ - æ›´æ–°æ–¼ 2025-07-25*